var tipuesearch = { "pages": [{"url": "/patterns/Icons-for-Privacy-Policies", "text": "Privacy\u00a0IconsServices (and products) which users use usually handle user data in ways which justify the use of a privacy policy. These documents are however made for legal purposes first and foremost and thus must cover a lot of detail rigorously. Users however are exposed to many of these documents when they seek to delve into the practices of each of the services they use. Controllers of these services realize the difficulty apparent in understanding full policy documents, but need users to understand risks if their processing consent is to be valid. Some approaches used to simplify policy are the layering of detail levels, general summarization, and contextual explanations. However, even these are subject to\u00a0shortcomings.Users struggle to understand privacy policies, even when reduced to a reasonable length. This discourages them from putting in the effort required to understand risks to their data, and invalidates\u00a0consent.Use privacy icons to aid in describing, grouping, and distinguishing the various policies in a privacy policy document. The icons should not allow for misinterpretation, which shall require user testing. Using consistent icons in a standardized way will promote\u00a0understandability.In this pattern, privacy icons should not be used in place of the full policy document, but used to augment it. They should be shown in a manner which explains the policy explanation, excerpt, summary, or full detail as appropriate. Examples of usage include describing kinds of data processed, means, purposes, and legitimate interests or other\u00a0justifications.This usage should aid users in determining not only whether to further explore a policy, but also a rough idea of what each policy entails. More than one icon may be used per policy to achieve this, so long as the content becomes less\u00a0complex.Icons should be consistent and yet distinguishable from one another. This may justify a certain size limit. The icons should also be self-explanatory. These aspects need verification from a representative sample of the user\u00a0population.Without dedicating too much effort, a user may quickly determine the potential risks of processing under a given policy. The user will be able to also quickly locate the other relevant policies both when first using a service and when revisiting\u00a0policy.When the icons are sufficiently standardized, or at least for the subset which are, the user will not first need to familiarize themselves with explanations. Where not the case, education can assist in changing this if the icons are indeed widely used and\u00a0consistent.The use of this measure will make policy more transparent, which will enhance the level of trust placed by users. Users which provided an invalid form of consent due to lack of policy understanding may then choose to retract it, or modify permitted\u00a0usage.Alice buys a fitness tracker and she is aware that the device collects her location, and sends it to a central web service in order to provide her with her fitness statistics (her fitness routes, the time spent...). [She immediately consents to this even though it asks to first read a privacy policy.] The device controller [consequently] aggregates this data and provides a business analytics service to third\u00a0parties.Alice is totally unaware of this secondary use of her data and may not agree to it. But accessing this policy involves accessing a website and going through a lengthy and legally oriented\u00a0document.Comparatively, the tracker could have provided a short policy summary on the packaging using icons to convey more information with less space. Alice would have noticed an icon she recognized to convey third party sharing. Curious of whom this third party might be, and what extra risks she might be taking, she searches the online policy and finds it to be a company she does not trust. As a result she would not have consented, and potentially not purchased the\u00a0device.S. Fischer-H\u00fcbner, C. K\u00f6ffel, J.-S. Pettersson, P. Wolkerstorfer, C. Graf, L. E. Holtz, U. K\u00f6nig, H. Hedbom, and B. Kellermann, \u201cHCI Pattern Collection - Version 2,\u201d\u00a02010.C. Graf, P. Wolkerstorfer, A. Geven, and M. Tscheligi, \u201cA Pattern Collection for Privacy Enhancing Technology,\u201d The Second International Conferences of Pervasive Patterns and Applications, vol. 2, no. 1, pp. 72\u201377,\u00a02010.European Parliament and Council of the European Union, \u201cGeneral Data Protection Regulation,\u201d Official Journal of the European Union,\u00a02015.", "tags": "", "title": "Icons for Privacy Policies"},{"url": "/patterns/Platform-for-Privacy-Preferences", "text": "Users are frequently intimidated or discouraged by the size and complexity of legal texts. Privacy policies are an example of such texts, which are in the user's best interest to understand. As these policies are also written for the sake of legal compliance, balancing or reconciling comprehensiveness with comprehensibility is nontrivial. Different users will have varying thresholds to the amount of detail they will readily look through. The controller in this case wants to make their privacy policy more accessible to their\u00a0users.Users regularly do not read privacy policies, as they are too verbose, complex, and repetitive amongst the sites they\u00a0visit.Controllers may use the P3P standardization of terms and data elements to construct their privacy policies, allowing users to instead immediately see the policy distinctions which matter before using the service. The policies they share with other controllers the user is subject to will already have been reviewed, or are separated such that minimal time is spent reviewing\u00a0policy.P3P uses eXtensible Markup Language (XML) to hold a variety of information concerning each web resource listed in a policy reference file. The XML includes the data elements or types collected, its recipients, and explanations of how each set of data is used (purposes and means). It also features important information about the controller and its general policies and practices, such as contact information, a link to the human readable policy, and dispute resolution. It does not contain information about what the controller does not\u00a0do.Further information is available at\u00a0https://www.w3.org/TR/P3P/Users will be able to construct preferences for a privacy standard (risk appetite) which they personally can accept. This template will allow them to quickly review the privacy policy of the controller while avoiding repetition, and understanding distinctions. They may additionally choose to have site-specific preferences which point out what is most relevant to\u00a0them.The human readable privacy policy should be compatible with what can also be expressed using the P3P standardization. While extensions can be made to the specification, there is a limit to this. Careful consideration will need to be used when constructing the policy to ensure full coverage. This may require additional explanation beyond what the P3P specification can provide, which needs to be clearly indicated and explained to\u00a0users.The following example is taken from the P3P1.0\u00a0specification:Claudia has decided to check out a store called CatalogExample, located at http://www.catalog.example.com/. Let us assume that CatalogExample has placed P3P policies on all their pages, and that Claudia is using a Web browser with P3P built\u00a0in.Claudia types the address for CatalogExample into her Web browser. Her browser is able to automatically fetch the P3P policy for that page. The policy states that the only data the site collects on its home page is the data found in standard HTTP access logs. Now Claudia's Web browser checks this policy against the preferences Claudia has given it. Is this policy acceptable to her, or should she be notified? Let's assume that Claudia has told her browser that this is acceptable. In this case, the homepage is displayed normally, with no pop-up messages appearing. Perhaps her browser displays a small icon somewhere along the edge of its window to tell her that a privacy policy was given by the site, and that it matched her\u00a0preferences.Next, Claudia clicks on a link to the site's online catalog. The catalog section of the site has some more complex software behind it. This software uses cookies to implement a \"shopping cart\" feature. Since more information is being gathered in this section of the Web site, the Web server provides a separate P3P policy to cover this section of the site. Again, let's assume that this policy matches Claudia's preferences, so she gets no pop-up messages. Claudia continues and selects a few items she wishes to purchase. Then she proceeds to the checkout\u00a0page.The checkout page of CatalogExample requires some additional information: Claudia's name, address, credit card number, and telephone number. Another P3P policy is available that describes the data that is collected here and states that her data will be used only for completing the current transaction, her\u00a0order.Claudia's browser examines this P3P policy. Imagine that Claudia has told her browser that she wants to be warned whenever a site asks for her telephone number. In this case, the browser will pop up a message saying that this Web site is asking for her telephone number, and explaining the contents of the P3P statement. Claudia can then decide if this is acceptable to her. If it is acceptable, she can continue with her order; otherwise she can cancel the\u00a0transaction.Alternatively, Claudia could have told her browser that she wanted to be warned only if a site is asking for her telephone number and was going to give it to third parties and/or use it for uses other than completing the current transaction. In that case, she would have received no prompts from her browser at all, and she could proceed with completing her\u00a0order.L. Cranor, M. Langheinrich, M. Marchiori, and J. Reagle, \u201cThe Platform for Privacy Preferences 1.0 (P3P1.0) Specification,\u201d W3C, 2002. [Online]. Available: https://www.w3.org/TR/P3P/. [Accessed:\u00a010-Oct-2017].O. Drozd, \u201cprivacypatterns.wu.ac.at - Privacy Patterns Catalog,\u201d privacypatterns.wu.ac.at, 2016. [Online]. Available: http://privacypatterns.wu.ac.at:8080/catalog/. [Accessed:\u00a025-Jan-2017].", "tags": "", "title": "Platform for Privacy Preferences"},{"url": "/patterns/Strip-invisible-metadata", "text": "Strip potentially sensitive metadata that isn't directly visible to the end\u00a0user.When a service requires a user to import data from external sources (eg.\npictures, tweets, documents) different types of metadata may be \ntransmitted. Users may not be aware of the metadata as it can be\nautomatically generated or not directly visible. Services might be\ninadvertently responsible for exposing private metadata, or going\nagainst users'\u00a0expectations.Users are not always fully aware of the various kinds of metadata\nattached to files and web resources they share with online services.\nMuch of this data is automatically generated, or not directly visible to\nusers during their interactions. This can create situations where, even\nthough users share information explicitly with services, they may be\nsurprised to find this data being revealed. In certain cases where the\ndata is legally protected, the service could be held responsible for any\nleakage of sensitive\u00a0information. How should services that need users to share data and upload files\ntreat additional metadata attached with files? In case of uploading\ndocuments and images, which parts of the metadata can be treated as\nexplicitly shared\u00a0information.Stripping all metadata that is not directly visible during upload time,\nor during the use of the service can help protect services from\nleaks and liabilities. Even in cases where the information is not\nlegally protected, the service can protect themselves from surprising\ntheir users and thus alienating\u00a0them. To summarize: user metadata that can not be made visible to users\nclearly should be stripped to avoid overstepping the users'\u00a0expectations. Twitter.com removes EXIF data from images uploaded to their image\n   sharing service. Previously there have been many breaches of personal\n   location by using EXIF data shared by image sharing\u00a0services. In certain cases services might build features based on\n   metadata, or the metadata sharing could be an important part of the\n   community of users. Flickr.com allows users to hide their EXIF data from\n   public display, and also provides an interface for users to easily see\n   whether they are sharing location as part of uploading their\u00a0images. TODO: add\u00a0screenshots", "tags": "", "title": "Strip Invisible Metadata"},{"url": "/patterns/Layered-policy-design", "text": "Layered Privacy Policies / Multi-Layered\u00a0NoticesAs the law in various parts of the world requires a number of considerations, policies tend to be long, complex documents which are difficult to understand. The same holds true for privacy, which supplies its own legislative concerns, particularly regarding data protection. The [data] controller in these instances, provides users (data subjects) with services (or products) to which privacy policies apply. These suffer the same detail rich and superfluous content pitfalls as other policies, though are legally required to be available to users in a manner which is both understandable and\u00a0complete.The controller needs to balance comprehension and comprehensiveness in their privacy policies in order to ensure that users choose to inform themselves. If they do not, then processing their information is\u00a0unlawful.Extract the most crucial aspects of the privacy policy, which users are most likely to read, to the foreground. Nest successive detail levels within these components so that users can quickly find information that is relevant to\u00a0them.A short notice may provide a summary of the practices that deal with personal data, highlighting those which may not be evident to the data subject. Then, a longer policy may provide specific information, split into sections, detailing any uses of personal data. And finally, the whole legal text of the privacy policy can be\u00a0specified.However, [multiple] versions of the privacy policies [need to] coexist, which may introduce potential contradictions; in particular, the data controller must ensure that updates are performed in parallel and\u00a0coherently.Pinnick, T. Layered Policy Design. TRUSTe Blog,\u00a02011.Christoph Boesch, Frank Kargl, Henning Kopp, and Patrick Mosby, \u201cprivacypatterns.eu - collecting patterns for better privacy,\u201d 2017. [Online]. Available: https://privacypatterns.eu/#/?limit=6&offset=0. [Accessed:\u00a018-Jul-2017].", "tags": "", "title": "Layered Policy Design"},{"url": "/patterns/Negotiation-of-Privacy-Policy", "text": "Often when users find a service (or product) they would like to use, and begin signing-up, they are immediately exposed to assumptions which may not hold for them. As users have differing privacy priorities, a controller cannot guess as to what settings best accommodate them. Since these preferences may be intricate, users cannot be expected to specify them in detail all at once or before using the\u00a0service.Users have sometimes wildly different priorities regarding their privacy, though a controller does not know these details when a user first joins a service. There is a temptation to provide these users the settings the average user\u00a0uses.As users begin to use a service, determine their individual privacy sensitivities by allowing them to opt-in/opt-out of account details, targeted services, and telemetry. When a user's preference is not known, assume the most privacy-preserving settings. It should always take more effort to over-share than to\u00a0under-share.Unauthenticated users should enjoy the most privacy-preserving defaults. When a user joins the service, they may be presented with [excerpts or summaries of] a privacy policy, which they can use to inform their choices. Using simple, recognizable controls, users can be asked to opt-in (for explained benefits) or opt-out (at explained costs) before any of their data is used. They can then be asked for additional consents further down the line as they become contextually\u00a0relevant.In this way, only the needed consent is asked for as the controller's understanding of the user's preferences improves. This can allow the service to determine which solicitations users are individually likely to consider, and which ones will only waste their time or upset\u00a0them.Private defaults will often not be the appropriate settings for a user, as most users may be less privacy-concerned. The additional effort taken to share more, with users or the controller, will reduce the valuable data collected. However, providing users with invasive defaults would risk public outrage by the vocal few, who may affect opinions\u00a0holistically.Based\u00a0on:J. Porekar, A. Jerman-Bla\u017ei\u010d, and T. Klobu\u010dar, \u201cTowards organizational privacy patterns,\u201d Proceedings - The 2nd International Conference on the Digital Society, ICDS 2008,\u00a02008.", "tags": "", "title": "Negotiation of Privacy Policy"},{"url": "/patterns/Encryption-user-managed-keys", "text": "Use encryption in such a way that the service provider cannot decrypt the user's information because the user manages the\u00a0keys.Enable encryption, with user-managed encryption keys, to protect the confidentiality of personal information that may be transferred or stored by an untrusted 3rd\u00a0party.User wants to store or transfer their personal data through an online service and they want to protect their privacy, and specifically the confidentiality of their personal information. Risks of unauthorized access may include the online service provider itself, or third parties such as its partners for example for backup, or government surveillance depending on the geographies the data is stored in or transferred\u00a0through. How can a user store or transfer their personal information through an online service while ensuring their privacy and specifically preventing unauthorized access to their personal\u00a0information?Requiring the user to do encryption key management may annoy or confuse them and they may revert to either no encryption, or encryption with the online service provider managing the encryption key (affording no protection from the specific online service provider managing the key), picking an encryption key that is weak, reused, written down and so\u00a0forth. Some metadata may need to remain unencrypted to support the online service provider or 3rd party functions, for example file names for cloud storage, or routing information for transfer applications, exposing the metadata to risks of unauthorized access, server side indexing for searching, or\u00a0de-duplication. If the service provider has written the client side software that does the client side encryption with a user-managed encryption key, there can be additional concerns regarding whether the client software is secure or tampered with in ways that can compromise\u00a0privacy.Encryption of the personal information of the user prior to storing it with, or transferring it through an online service. In this solution the user shall generate a strong encryption key and manage it themselves, specifically keeping it private and unknown to the untrusted online service or 3rd\u00a0parties.", "tags": "", "title": "Encryption with user-managed keys"},{"url": "/patterns/Privacy-icons", "text": "A privacy policy which is hard to understand by general audience is summarized and translated into commonly agreed visual icons. A privacy icon is worth a thousand-word\u00a0policy.This pattern can be applied to any system which collects end user data. It can be presented in an interactive web page but also as part of a physical product which can collect data (e.g. fitness\u00a0tracker)Many organizations provide privacy policies which are too lengthy and hard to understand by the general audience. These policies are oriented as legal disclaimers for legal issues, rather than to inform end users so they can consent to the organization practices after being clearly informed of the collected data, its purpose, and the processing and potential sharing with third\u00a0parties.Include within the service/device a very accessible and visual explanation of the privacy policy. Icons are a great complement to written text, as they may convey much information at a glance through a different modality (images). Standardized icon sets may thus be added to the privacy\u00a0policy.Truly inform customers of the privacy policy of a\u00a0system/organizationUsers may understand, at first glance, what are the potential risks of consenting of a privacy policy. In order to be useful, the icons must be well known and understood by the majority of the potential users before being used. A common meaning of the icon needs to be shared by the community. Educational material can be built upon the implications of each of these\u00a0icons.Alice buys a fitness tracker and she is aware that the device collects her location, and sends it to a central web service in order to provide her with her fitness statistics (her fitness routes, the time spent...). The device provider aggregates this data and provides a business analytics service to third\u00a0parties.Alice is totally unaware of this secondary use of her data and may not agree to it. But accessing this policy involves accessing a website and going through a lengthy and legally oriented\u00a0document.Currently, most of these are only applied by client-side\u00a0solutions.See also the Privacy Icons entry at Ideas for a Better Internet (kind of a pattern repository by the Berkman Center for Internet and Society in\u00a0Harvard).", "tags": "", "title": "Privacy icons"},{"url": "/patterns/Policy-matching-display", "text": "Personalized Policy Matching\u00a0DisplayControllers have policies written in a manner appropriate for legal evaluation, as it is the legal compliance which warrants them in the first place. Users tend to not be able to comprehend such language, and do not typically care to spend the time and effort required to parse it. However, much of the content in these policies is consistent throughout the services they\u00a0use.Users value using a service (or product) without having to go through repetitive and verbose policy detail. However, these users must still understand the policies which apply to them in order to not be blindsided. Controllers need to avoid this as keeping users happy is integral to a sustainable business\u00a0model.Users may get overwhelmed by the complexity of policies impacting privacy when using a service, compromising the validity of their informed\u00a0consent.Retrieve user policy preferences and use these to highlight contradictions with the privacy policy. Where possible, configure application settings to the values which best adhere to these\u00a0preferences.On the other hand, if the notification is not noticeable, the user may overlook an important policy distinction. Notifications which are persistent or ubiquitous may quickly desensitize users, and should also be used with\u00a0care.Allows users to provide a consistent privacy threshold while reducing cognitive workload as they use\u00a0services.S. Fischer-H\u00fcbner, C. K\u00f6ffel, J.-S. Pettersson, P. Wolkerstorfer, C. Graf, L. E. Holtz, U. K\u00f6nig, H. Hedbom, and B. Kellermann, \u201cHCI Pattern Collection - Version 2,\u201d\u00a02010.C. Graf, P. Wolkerstorfer, A. Geven, and M. Tscheligi, \u201cA Pattern Collection for Privacy Enhancing Technology,\u201d The Second International Conferences of Pervasive Patterns and Applications (Patterns 2010), vol. 2, no. 1, pp. 72\u201377,\u00a02010.", "tags": "", "title": "Policy Matching Display"},{"url": "/patterns/Enable-Disable-Functions", "text": "Users frequently have data collected about them, often in situations where it needn't be. Many of these cases are due to good intentioned, expansive, functionality. Not all users seek to take advantage of all functions, however. Some controllers aim to consider this in their\u00a0designs.Not all users desire or benefit from all\u00a0functionality.Consider users living in an Ambient Assisted Living environment: these users are surrounded by various sensors such as video cameras, motion sensors or electrical current sensors that are used to monitor the actual situation of a person. Another example are the acceleration sensors included in smartphones. A [service (or product)] can recommend places of interest to the user by considering the gathered [data]. With regard to these examples it becomes obvious that [services] often unobtrusively collect highly critical and personal context data of\u00a0users.Enable users to choose which functions they do not consent to using, nor wish to provide the required data\u00a0for.A solution is given if the user can explicitly agree or disagree to certain functions. For this purpose, the [service] has to display every function and its required context data. A possible way of displaying these functions and the used context data may be the use of the privacy consent form, which is included in every\u00a0application.By enabling the user to explicitly agree or disagree to certain functions, a context aware application like Support-U might not be able to provide all of its possible functionalities to the user anymore. However, the usage of this pattern in the development process of context-aware applications might additionally strengthen the user's confidence in the usage of UC\u00a0systems.In the shown privacy consent form each function, which utilises personal context information, is listed. Furthermore, the user is able to activate or to deactivate the functions, e.g., to enable a live stream or to enable predicting her next\u00a0context.Meet-U provides several functions that make use of localization mechanisms and the personal data the user supplies. That includes the user's interests, buddy list and [their] preferred means of transportation. For indoor navigation a RFID sensor attached to the user is exploited. The user can now switch off the navigation function so that neither the indoor nor the outdoor localization continue to operate. The user's preferences concerning transportation will be no longer available. Further functions can be disabled correspondingly. Turning off, for example, the advanced search engine would stop using the user's\u00a0interests.Based\u00a0on:", "tags": "", "title": "Enable/Disable Functions"},{"url": "/patterns/Sign-an-Agreement-to-Solve-Lack-of-Trust-on-the-Use-of-Private-Data-Context", "text": "Suggested: Contractual\u00a0ConsentThe controller does not necessarily have the trust of its users, and needs this trust for its services to process their\u00a0data.The service should provide the user with a contractual agreement (featuring privacy policy) which binds the controller to their word, provided that the user consents to the processing of data needed for specific purposes. The agreement should also bind any representative of the controller. It should be straightforward and clear enough for the user to\u00a0comprehend.The service should feature a mechanism (e.g. landing page or unavoidable introduction) prior to collection, which stipulates the need for user consent. There should be a reasonable effort to prevent users from bypassing this\u00a0mechanism.The specific purposes for which their data will be processed should be made clear. The service should, at the same time, outline the contractual obligations it will be held against should the user consent. The user should be able to seek further detail about these obligations without first needing to\u00a0consent.If users decide to consent, they can make this clear by interacting with a mechanism (e.g. button) which clearly represents their agreement to the\u00a0contract.A further implementation could additionally allow the user access to a subset of the service which does not require any data, in order to help justify their consent. This would also alleviate the user's potential apprehension about the time taken to review and inform themselves about their\u00a0decision.The controller, any of their representatives, and their users are tied to the terms of the contract and the legal implications it holds. Any disputes will involve both contract law and privacy\u00a0law.Users may be discouraged to use a service if they are made aware of the risks to their privacy, or introduced to the ways in which their data can be used to reveal\u00a0information.They may also be tempted to consent without reading about the contract or how their data may be used. Therefore it is useful to not force an immediate decision, as this can invalidate the consent as not freely given or\u00a0uninformed.Based\u00a0on:", "tags": "", "title": "Sign an Agreement to Solve Lack of Trust on the Use of Private Data Context"},{"url": "/patterns/Privacy-dashboard", "text": "A service (or product) which processes personal data of users may make that data accessible to them. This is often the case whether conforming to laws about self-determination and notice, or merely wanting to provide an additional privacy consideration for the sake of users. The controller concerned wants to open up about the data they have processed, and to improve the ease of use for configuring privacy\u00a0settings.A system should succinctly and effectively communicate the kind and extent of potentially disparate data that has been\u00a0processed.Users may not remember or realize what data a particular service or company has collected, and thus can't be confident that a service isn't collecting too much data. Users who aren't regularly and consistently made aware of what data a service has collected may be surprised or upset when they hear about the service's data collection practices in some other context. Without visibility into the actual data collected, users may not fully understand the abstract description of what types of data are collected; simultaneously, users may easily be overwhelmed by access to raw data without a good understanding of what that data\u00a0means.Provide successive summaries of collected or otherwise processed personal data for a particular user, representing this data in a meaningful way. This can be through demonstrative examples, predictive models, visualizations, or\u00a0statistics.Where users have choices for deletion or correction of stored data, a dashboard view of collected data is an appropriate place for these controls (which users may be inspired to use on realizing the extent of their collected\u00a0data).A variation of the privacy dashboard, Privacy Mirrors, focuses on history, feedback, awareness, accountability, and\u00a0change.Implementing this pattern is a matter of providing logging, reporting, and other informational access and notifications on user-selected/filtered, appropriately defaulted, relevant usage\u00a0data.As in other access mechanisms, showing a user's data back to them can create new privacy problems. Implementers should be careful not to provide access to sensitive data on the dashboard to people other than the [user]. For example, showing the search history associated with a particular cookie to any user browsing with that cookie can reveal the browsing history of one family member to another that uses the same\u00a0computer.Google Accounts: About the\u00a0DashboardDashboards are a widely-used pattern in other data-intensive activities for providing a summary of key or actionable\u00a0metrics.D. H. Nguyen and E. D. Mynatt, (2002). \u201cPrivacy Mirrors : Understanding and Shaping Socio-technical Ubiquitous Computing Systems', pp.\u00a01--18.S. Romanosky, A. Acquisti, J. Hong, L. F. Cranor, and B. Friedman, (2006). \u201cPrivacy patterns for online interactions,\u201d Proceedings of the 2006 conference on Pattern languages of programs - PLoP \u201906, p.\u00a01.E. S. Chung et al., (2004). \u201cDevelopment and Evaluation of Emerging Design Patterns for Ubiquitous Computing,\u201d DIS '04 Proceedings of the 5th conference on Designing interactive systems: processes, practices, methods, and techniques, pp.\u00a0233--242.H. Baraki et al., (2014). \u201cTowards Interdisciplinary Design Patterns for Ubiquitous Computing Applications,\u201d Kassel,\u00a0Germany.G. Iachello and J. Hong, (2007). \u201cEnd-User Privacy in Human-Computer Interaction,\u201d Foundations and Trends in Human-Computer Interaction, vol. 1, no. 1, pp.\u00a01--137.", "tags": "", "title": "Privacy dashboard"},{"url": "/patterns/Increasing-Awareness-of-Information-Aggregation", "text": "Controllers process mass amounts of user data in order to provide enhanced services (or products). Aggregating this data with other sources unlocks new insights which could not be determined alone. This kind of aggregation is distinct from the notion of abstracting information away from personal data, effectively making it less sensitive. Instead this may turn seemingly harmless data into identifying, intrusive, or inferred information, some of which not even the user is aware of. This makes aggregation very useful for marketing, as well as other more usability-centric features, but places a heavier burden on users to disclose with\u00a0care.Poor awareness of data aggregation capabilities can lead to unintentionally revealing information being disclosed. Processing this personal data goes against the principles of data\u00a0protection.Provide users with knowledge of data aggregation's ability to reveal undesirable information to prevent them from over sharing. Take users through a hypothetical example to aid in conveying\u00a0this.Prior to allowing users to submit content to the controller or other parties, provide them with a warning about data aggregation. This warning is only necessary where aggregation is applied. As such if it is determined after collection that data should be aggregated, this warning would be given prior to obtaining consent for that further\u00a0processing.The warning must make it clear to the user that content they disclose may be more sensitive that it first appears. The context in which they provide it may be subject to changes, and these potential contexts should be provided to the user, or else consented to as they become applicable. The user should not have to deal with broad or otherwise unclear usage of their\u00a0data.At the same time, the user should not be exposed to deep, complex, and lengthy detail unless they choose to review it further. Instead, concise and clear explanations should be used. One approach to this is to provide a hypothetical example in which a controller reveals surprising characteristics about a user from combinations of data, which alone are less\u00a0informative.Consider the use of user tests to determine the level of clarity an explanation or example provides. It is important that if a user chooses to accept the risks (and benefits) of aggregation, then they do so knowingly. It is also important not to force aggregation onto users if they choose not to consent. This may prevent the user from gaining a feature, but should not lock them out of functionality which does not require\u00a0it.If users understand the power of data aggregation better, they are better able to put any new data they're about to share in perspective to all the data they've already shared, and may consider the total picture this creates of them more carefully. But this also means that it becomes harder for organizations to create accurate profiles of people and may result in improper labeling based on the little data that is\u00a0known.S. Ahern, D. Eckles, N. Good, S. King, M. Naaman, and R. Nair, \u201cOver-Exposed ? Privacy Patterns and Considerations in Online and Mobile Photo Sharing,\u201d CHI \u201907, pp. 357\u2013366,\u00a02007.", "tags": "", "title": "Increasing awareness of information aggregation"},{"url": "/patterns/Whos-Listening", "text": "User\u00a0ListUsers of a service regularly share its usage with other users. Sometimes these are users they know personally, and sometimes these are anonymous, unauthenticated persons. This occurs particularly in shared or collaborative environments where content is generated. Knowledge of the contributions of other users contributes to additional or refined content in general. Controllers facilitating this interaction therefore encourage the users to form groups or allow public access. Though when the amount of disclosure is high it is difficult to keep track of attribution and\u00a0modification.Users do not know if the content they are accessing or have disclosed has been accessed or modified by others, nor if it is someone they\u00a0know.Provided that users know their access is not private, inform them of other users, even unauthenticated, which are also accessing the content in\u00a0question.Ensure that it is made clear to the user that the content they are about to view is accessed in a shared and public manner. Their access will be visible to others, and may be recorded by the system (if applicable) for historic views, or for preventing\u00a0abuse.The implementation of the system prior to this will likely only require the addition of UI elements to indicate the access state as the system already perceives it. Each user may be shown using some identifier easily recognizable by other users, such as a randomly selected avatar (e.g. Gravitar), initial(s), username, or profile picture. The same may identify unauthenticated users as\u00a0'anonymous'.Where historic views are provided, the same consistent identifier can be shown next to differential changes along with timestamps. The ability to edit, remove, or anonymize a contribution may also be available if desired. Details of these extra features, or justification for the lack of user ability to perform these actions, should be provided prior to\u00a0usage.This pattern will only work, if the users trust the system that provides the information and log in personally. In web based systems that don\u2019t require personal login, it is not possible to [reliably] detect, who is visiting the site (even cookies [and browser fingerprints] do not [necessarily] reveal information about the users\u2019 identities). This is problematic for [attribution], but it ensures that the users can control their\u00a0privacy.The BSCWshared workspace system (Bentley, Horstmann, and Trevor 1997) [logs] accesses to the shared [content]. The event log can be queried [by users] and for each document stored in the shared workspace, the users can define notification patterns. By these means, it is possible for an author of a document to find out who read the document (and\u00a0when).Various collaborative environments, like Google's Docs, or chat rooms, instant messaging, and other immediate content sharing mediums frequently provide lists of currently online users. These can also indicate a number of anonymous users who have not authenticated, but have reduced\u00a0privileges.T. Sch\u00fcmmer, \u201cThe Public Privacy - Patterns for Filtering Personal Information in Collaborative Systems,\u201d in Proceedings of CHI workshop on Human-Computer-Human-Interaction Patterns,\u00a02004.R. Bentley, T. Horstmann, and J. Trevor, \u201cThe world wide web as enabling technology for cscw: The case of bscw,\u201d\u00a01997.", "tags": "", "title": "Who\u2019s Listening"},{"url": "/patterns/Pseudonymous-identity", "text": "Hide the identity by using a pseudonym and ensure a pseudonymous\nidentity that can not be linked with a real identity during online\u00a0interactions.This pattern can be used for systems in which users are identified by\npublic\u00a0identities.Many kinds of sensitive informations are released through web\ninteractions, email, data sharing or location-based systems, which can\ncontain the name of a user or header information in packets. Another\nproblem could be to interact anonymously in a forum. However too much\ninteraction in a forum with an anonymous identity can be dangerous in\nthe sense that the relation between original identity and a\npseudonymous identity can be\u00a0exposed.Initiate a random pseudonym, that can not be related to the original,\nso that the identity is hidden. Furthermore a pseudonym depends on\nconcealment, so the pseudonym allocation needs\u00a0protection.Hide the identity of the\u00a0participants.The real identity of a user is hidden. In certain scenarios there is a\nneed for additional space to store the pseudonym-identity mapping.\nExtensive Usage of the same pseudonym can weaken\u00a0it.Assuming some students are writing an exam and they have to fill out a\nform about their identity, where there is an optional field for a\nchosen pseudonym. This way the result can be released under the chosen\npseudonyms and the identity of each student is hidden. But by being\nobservant, some students might be able to figure out which identity\nbelongs to which pseudonym and so the confidentiality of the identity\nis\u00a0compromised.Anonymizer are well-known tools for anonymous web interactions. They\nwork for example by using a proxy between a request sender and a\nrecipient to strip header information like HTTP_USER_AGENT in packet\nheaders because they contain metadata about packet senders. The\nMixmaster is an anonymous remailer that hides the sender and recipient\nidentity by stripping its name and assigning a pseudonym. Some data\nsharing systems with a privacy-preserving focus make use of pseudonyms\nso that identifying information such as names and social security\nnumbers are hidden. For example various electronic healthcare systems\nare using pseudonyms for the storage of e-health\u00a0records.", "tags": "", "title": "Psuedonymous Identity"},{"url": "/patterns/Use-of-dummies", "text": "This pattern hides the actions taken by a user by adding fake actions\nthat are indistinguishable from\u00a0real.This pattern is applicable when it is not possible to avoid executing,\ndelaying or obfuscating the content of an\u00a0action.When users interact with ICT systems their actions reveal a lot of\ninformation about themselves. An option would be for users to not\nperform such actions to protect their privacy. However, this is not\npossible since users cannot completely avoid executing these actions\nbecause they need to perform them to achieve a goal (e.g., search for\na word on the Internet, send an email, search for a\u00a0location).Since the action must be accurately performed, an option to provide\nprivacy is to simultaneously perform other actions in such a way that\nthe adversary cannot distinguish real and fake (often called dummy)\u00a0actions.To hinder the adversary\u2019s ability to infer the user behavior, as well\nas her\u00a0preferences.This pattern entails the need for extra resources to perform the dummy\nactions, both at the side of the user that must repeat the action, and\nat the server side that must process several actions. Sometimes it may\ndegrade the quality of service since the service provider cannot\npersonalize services. It has been demonstrated that generating dummies\nthat are perfectly indistinguishable from real actions (in terms of\ncontent, timing, size, etc...) is very\u00a0difficult.Alice wants to search for an abortion clinic on Google, but she does\nnot want to reveal her intentions of abort to an adversary that may be\neavesdropping this search (e.g., ISP provider, system administrator of\nher workplace,\u00a0etc).The use of this pattern has been proposed to protect privacy in\nlocation based services (the user reveals several locations to the\nservice provider so that her real location is hidden), anonymous\ncommunications (the user sends fake messages to fake recipients to\nhide her profile), web searches (the user searches for fake terms to\nhide her real\u00a0preferences).", "tags": "", "title": "Use of dummies"},{"url": "/patterns/Support-Selective-Disclosure", "text": "Many services (or products) require the collection of a fixed, often large, amount of personal data before users can use them. Many users, instead, want to freely choose what information they share. This pattern recommends that services Support Selective Disclosure, tailoring functionality to work with the level of data the user feels comfortable\u00a0sharing.Controllers aim to design services to be both maintainable and extensible, though as a result blanket strategies are used to simplify designs. Users are individuals and do not always respond the same way to different approaches. Restricting user choice on processing displeases users, and bundling purposes for that processing conflicts with international law. Users want a service which works without the data they do not want to provide, even so far as effectively anonymous\u00a0usage.Controllers typically want to collect data by default, and tend to limit the diversity of their services, and the choices they provide, to encourage that. This goes against the best interests of the users, who have varying data collection\u00a0tolerances.The underlying issues are discussed in more detail\u00a0below.Controllers are tempted to see consent as all-encompassing, see held personal data as data available for use, and the lack of that data as a barrier to service. This mindset reduces adoption of the offering and may introduce a lack of\u00a0trust.This problem is also present when users register for or acquire a service, as unnecessary information is often requested as part of the process.  In the case of account registration users are often provided with inappropriate default settings. They are typically sent additional offers by default as well. The negative implications of these defaults are also not necessarily reversible, as the Internet is notorious for its inability to\u00a0forget.Services tend to collect a surplus of information, especially in contexts where monitoring is integral to the system, such as in productivity tracking. This unnecessary level of detail results in negative experience factors for the tracked individuals (for e.g. increased levels of anxiety) which in a work environment may affect their actual\u00a0productiveness.Determine what information is integral to the functioning of the system. If functionality may be sustained with less, it should be an option for the user, even if doing so comes with reduced usability. Additionally, provide anonymous functionality only where it cannot jeopardise the service. Lower levels of anonymity may be provided in relation to various capabilities for\u00a0abuse.At one extent it may be possible to benefit from the system anonymously, though whether this is feasible will depend on the level of abuse anonymous usage might attract. Alternatively, this can be approached from the perspective of revocable privacy. That is, tentative or eroding anonymity. If this would result in an unsustainable business model, however, a re-balance of usability may be\u00a0sufficient.It is important to note that while anonymous usage might not translate into direct profit, additional contributors and positive public perception may increase overall user activity. Furthermore, there are payment methods which support [some level of] anonymity if\u00a0necessary.Where users choose to register, it should not be assumed that they wish to use all of the system's services. Short of explicitly opting for 'best experience settings' (with sufficient explanation; not the default option), user preferences should default to the most privacy-friendly\u00a0configuration.User decisions should be amendable. For example, an agreement to share activity with another user may not carry over to all future usage. A user may decide to share something once in a while, or share regularly, but not always. The system should be able to account for this behaviour if it aims to prevent mistaken\u00a0actions.In situations where there are requirements for personal data, particularly when strict, users should be aware of this prior to their consent. These services should also not be coupled with other services holding lower requirements unless it would be infeasible not to. Where users are required to use the system, no unnecessary information should be used. In a productivity tracking example, this may mean that users are only identified when their productivity falls, or perhaps if they opt to receive credit for their\u00a0work.Due to increased control over their data, users may be able to share pieces of information which they otherwise wouldn't due to it otherwise being coupled with what they perceive to be more\u00a0sensitive.Users will be less likely to mistakenly release personal information to the public, since they would perhaps be able to set their own defaults, or by default stay private. To a further extent, users may be capable of participating or benefiting from a system anonymously. Where this is the case, the activity levels of the system will benefit, and users who stayed anonymous due to mixed feelings about the system may decide to register and authenticate later, once trust has been\u00a0built.The system's complexity will increase by a certain degree, as not only will each user need to have their preferences set, stored, and adhered to, but also services will need to account for variable inputs. As such, flaws in the system will be felt with greater\u00a0effect.Providing anonymity for some contexts may result in increased undesired behaviour, depending on the level of anonymity provided. Anonymising a service often requires additional processing power, especially in the case of revocable\u00a0privacy.By separating functionality according to purpose and personal data needed, as well as providing variations where feasible, the system will be more complex. Services will need to be designed while taking into consideration the potential for limited access to\u00a0data.Improvements to results may therefore be limited as well. However, the controller may be able to gauge adoption in data-rich services while they are investing in them. The same holds for determining how valuable non-invasive alternatives are, as users will express their [in]tolerance for invasiveness through their\u00a0actions.S. Ahern, D. Eckles, N. Good, S. King, M. Naaman, and R. Nair, \u201cOver-Exposed ? Privacy Patterns and Considerations in Online and Mobile Photo Sharing,\u201d CHI \u201907, pp. 357\u2013366,\u00a02007.H. Baraki et al., Towards Interdisciplinary Design Patterns for Ubiquitous Computing Applications. Kassel, Germany,\u00a02014.E. S. Chung, J. I. Hong, J. Lin, M. K. Prabaker, J. a. Landay, and A. L. Liu, \u201cDevelopment and Evaluation of Emerging Design Patterns for Ubiquitous Computing,\u201d DIS \u201904 Proceedings of the 5th conference on Designing interactive systems: processes, practices, methods, and techniques, pp. 233\u2013242,\u00a02004.G. Iachello and J. Hong, \u201cEnd-User Privacy in Human-Computer Interaction,\u201d Foundations and Trends in Human-Computer Interaction, vol. 1, no. 1, pp. 1\u2013137,\u00a02007.J. Porekar, A. Jerman-Bla\u017ei\u010d, and T. Klobu\u010dar, \u201cTowards organizational privacy patterns,\u201d Proceedings - The 2nd International Conference on the Digital Society, ICDS 2008,\u00a02008.S. Romanosky, A. Acquisti, J. Hong, L. F. Cranor, and B. Friedman, \u201cPrivacy patterns for online interactions,\u201d Proceedings of the 2006 conference on Pattern languages of programs - PLoP \u201906, p. 1,\u00a02006.T. Sch\u00fcmmer, and J. M. Haake (2001). \u201cSupporting distributed software development by modes of collaboration,\u201d in Proceedings of ECSCW 2001,\u00a0Bonn.T. Sh\u00fcmmer, \u201cThe Public Privacy \u2013 Patterns for Filtering Personal Information in Collaborative Systems,\u201d in Proceedings of CHI workshop on Human-Computer-Human-Interaction Patterns,\u00a02004.", "tags": "", "title": "[Support] Selective Disclosure"},{"url": "/patterns/Discouraging-blanket-strategies", "text": "Socially oriented services on the Internet allow their often diverse userbase to share content. These masses of users and shared content are also varied enough to discourage individual attention. Controllers prefer to protect themselves from additional complexity and investment into features which provide them with less data. Users, however, feel in need of privacy settings to distinguish their personal risk appetite from that of the norm. They each have their own ideas about the sensitivities of their information, which makes sufficient controls difficult to\u00a0implement.Overly simplified privacy settings following all or nothing strategies could result in over-exposure, self-censoring, and unsatisfied\u00a0users.These all or nothing strategies could refer to privacy settings which holistically apply to all content, or to binary (or otherwise deficient) choices for public\u00a0visibility.Provide users with the possibility to define a privacy level for content being shared with the controller, or with other users. Give them a range of visibilities, so that they can decide the access-level of the content being shared according to different users, or service-defined\u00a0groups.Provide users easily-recognizable visual elements to define the privacy level for each content submission. Use controls, such as (drop-down) lists, combo boxes, etc. to provide a range of possible privacy\u00a0levels.The privacy controls themselves also need to be designed in such a way that it is very clear to users what each setting does and what it means for their\u00a0privacy.Based\u00a0on:S. Ahern, D. Eckles, N. Good, S. King, M. Naaman, and R. Nair, \u201cOver-Exposed ? Privacy Patterns and Considerations in Online and Mobile Photo Sharing,\u201d CHI \u201907, pp. 357\u2013366,\u00a02007.", "tags": "", "title": "Discouraging blanket strategies"},{"url": "/patterns/Privacy-aware-network-client", "text": "Privacy Policy\u00a0ProxyUsers frequent various domains across the Internet for a number of reasons, each controller having their own set of policies to which users are held. The need for users to understand these policies is shared by most controllers, as catering to an international audience enhances the legislative impact. One such impact is the need for informed consent. Not understanding the purposes and means for personal data processing invalidates consent, which makes that processing unlawful. There are however numerous other requirements for these controllers, which tend to make their policies verbose and\u00a0complex.Privacy policies are typically written to satisfy legal requirements ahead of conveying concise and easily understandable information to users. This makes users less informed\u00a0overall.Provide a privacy preserving proxy which securely parses and interprets the privacy policies of controllers, supplying users with standardized and easily understood summaries of those\u00a0policies.Figure 1 [of the paper] shows a class diagram for the relationships between the user, the server, and the proxy. Each server can publish many policies and each user can be made aware of many policies at a time through the\u00a0proxy.Design and implement a proxy able to parse and interpret privacy policies written in some standard language. This proxy could be built as a specialized version of the Proxy pattern [by the Gang of Four]. The proxy could be able to interpret several privacy languages or just one of them. Successful use of the pattern requires that the proxy can understand the server\u2019s privacy\u00a0language.Design and implement a secure communication channel between network clients and their proxies. This is necessary to avoid interception of the user choices by malicious\u00a0[actors].The user's awareness of the privacy policy rises so that more informed decisions can be made. The proxy is able to automatically detect changes of the privacy policy. A separate secure connection is needed for the proxy for every access to an area which is secured by a privacy policy. Policy constraints need to allocate local storage in the client. An attack on this could lead the user to decisions which [they] would otherwise not do. If there are any breaches of privacy it can be blamed on the client if [they] did use a privacy-aware client for a particular\u00a0access.Alice uses several web-based services but is not aware of the their privacy policies. Even when she reads the policies, she is still not aware of the actual implications of the legal description. In the absence of other solutions, she does not read the policies and does not understand the\u00a0ramifications.JRC P3P Proxy Version 2.0 is a P3P user agent acting like an intermediary. Depending on the specified privacy preferences of a user, it controls the access to web servers. Another known P3P user agent is AT&T Privacy Bird. Privacy Bird is a tool warning users if privacy policies of visited websites are not matching with their individual privacy\u00a0preferences.M. Sadicoff, M. M. Larrondo-Petrie, and E. B. Fernandez, \u201cPrivacy-aware network client pattern,\u201d in Proceedings of the Pattern Languages of Programs Conference,\u00a02005.\u201cprivacypatterns.eu - collecting patterns for better privacy,\u201d privacypatterns.eu, 2017. [Online]. Available: https://privacypatterns.eu/. [Accessed:\u00a020-Oct-2015].", "tags": "", "title": "Privacy-Aware Network Client"},{"url": "/patterns/Reasonable-Level-of-Control", "text": "Users have certain expectations about what level of privacy they can expect in certain contexts. In general, they are given the means to provide themselves with as much or little shielding from intrusions as they need. This expectation carries over to usage of services (or products) offered by a Controller. Users expect that they can have an impact on what about them is known to a service, or others that use the\u00a0service.Users expect to be afforded sufficient self-determination over what information about them is collected or otherwise processed. The level of information and control desired, however, varies from person to person, as does the negative response when expectations are not\u00a0met.Allow users to selectively and granularly provide information to a service, or its users, and have select information available to user-defined or predetermined\u00a0groups.Users should be able to push their chosen information to (or have it pulled by) those they grant access. Using push mechanisms, users will have the greatest level of control due to the fact that they can decide the privacy level of their data case by\u00a0case.Pull mechanisms are less granular, as granting access to a group or individual continues until that access is denied. Within this time frame, the sensitivity of the data may fluctuate. However, the user should have the ability to retract access at will, and thus, can manage their own identified\u00a0risks.Elsewhere, ensure that any required fields are truly required, and that the completeness needed for those fields be indicated. When there are automatic suggestions, let users redefine or remove the information before it is collected by the service. These automatic suggestions should also not take place without\u00a0consent.Where information is provided on a continual basis to those granted access, provide the user with the necessary tools. They should be able to indicate who falls within a group, and what exactly that group can access, for how long, at what granularity, how far back they can look, and so\u00a0forth.Based\u00a0on:", "tags": "", "title": "Reasonable Level of Control"},{"url": "/patterns/Selective-Access-Control", "text": "Selective Access Control in Forum Software;\nPrivacy Options in Social\u00a0NetworksUsers want to control the visibility of the content being shared, because it may not currently be appropriate for all\u00a0users.Provide users with the option to define the audience of their contributions by specifying the access rules to their\u00a0[content].Implement visual controls to help users to define access control rules when they create or modify\u00a0content.These rules could be defined based on users, groups of users, or based on context-aiding attributes like age or location. For groups, it should be possible to directly define who may view the post being published (e.g. a post with personal data aimed only at a group of close friends). Contextually, it may be possible to define an attribute constraint based on whom in general the post is intended for (e.g. a post aimed at people in a specific town or\u00a0region).Based\u00a0on:Drozd, O. (n.d.). Privacy Patterns Catalog. Retrieved January 25, 2017, from\u00a0http://privacypatterns.wu.ac.at:8080/catalog/Fischer-H\u00fcbner, S., K\u00f6ffel, C., Pettersson, J.-S., Wolkerstorfer, P., Graf, C., Holtz, L. E., \u2026 Kellermann, B. (2010). HCI Pattern Collection \u2013 Version\u00a02.", "tags": "", "title": "Selective access control"},{"url": "/patterns/Active-broadcast-of-presence", "text": "Controllers provide an interface for acquiring information about the user. When one such user wants to share or broadcast their information, such as location or other presence data, that user may want to constrain the information. In this way, they may wish to prioritize data that is contextually relevant, or avoid a full stream of data which may be either noisy or intrusive. The controller wants the user to be able to provide this data at will, to maximize the applicability of their services. However, they do not want the user to regret providing too much data, nor to bother the user with constant\u00a0requests.A service aims to acquire or broadcast a user's real-time data, particularly presence or location information, to a platform (e.g. social network). They wish to do so without revealing sensitive data (e.g. private locations, histories, or health information) nor overwhelming recipients with noisy data or users with constant\u00a0requests.The service may present distinct contexts in which to honor explicit settings, but in absence of this context assume that further consent is required. The user may choose not be be asked again, but must make this decision\u00a0explicit.In addition to privacy settings with appropriate defaults, allow the user the option to be asked again, every time the context\u00a0changes.By default, users should actively choose to broadcast rather than the service deciding based on general settings which may not apply to the present context. Various contexts may be provided distinct\u00a0settings.In these situations users need only be reminded prior to setting the values themselves. After this, they may choose to be notified about broadcasting, but not about sharing with the service itself. In this way, the user may decide\u00a0later.Based\u00a0on:", "tags": "", "title": "Active broadcast of presence"},{"url": "/patterns/Abridged-Terms-and-Conditions", "text": "Controllers which provide services (or products) to users have various policies, including those which affect user privacy, which need to relayed to the user. If users do not have knowledge of the risks, rights, and responsibilities relevant to them, this is the fault of the controller. Keeping users (the data subjects) informed, especially prior to acquiring consent, is a legal requirement. As such, controllers need to ensure that this is the case. It is however difficult to keep user attention on such matters, as they are often more willing to spend time on other things, including actually using a system. Efforts to hold attention by force also face active\u00a0resistance.Users often overlook Terms and Conditions when presented with them in their entirety before the use of a\u00a0service.Summarize the legally sufficient Terms and Conditions into concise and relevant variations which suit the user's level of interest and attention. At first use of a service, users should be able to investigate further, but not have to read much to understand the risks\u00a0involved.Prepare the concise Terms and Conditions according to a user perspective, focusing on matters which are most important to them. Aspects which do not affect them should not be included in summarized variations. Where areas of potential interest are easily bundled, group them under a general summary with option to expand further. Using titles in this regard is less helpful if they do not summarize the policies involved, as expanding should not be necessary unless the user notes an area worth their concern. Aim towards a page or less of information, as the inclusion of a scrollbar may dissuade the\u00a0user.The full, legally sound version should also be available, and should not contradict the summarized information. This applies at first use as well, as a user should be allowed to review detail prior to being subjected to\u00a0it.The appropriate and concise summarization of the Terms and Conditions will allow users to get a sufficient idea of the rights, risks, and responsibilities relevant to them. As it should be brief, only the most carefree users will overlook them. It will not therefore be guaranteed that users are fully informed, and this should be taken into\u00a0account.Due to the fact that the [Terms and Conditions] of an application are condensed to a size that is easily comprehensible, a user\u2019s trust in the application can be increased. [Additionally, this] ensures a greater transparency to the user since possible implications for the user, which may result through the usage of the application, can be recognized more easily\u00a0beforehand.The relationships this pattern has to these accessible policies patterns stem from its inherent compatibility with policy standardization, summarization, separation into layers and re-wording. Abridging a policy may support these aspects, or even entail them, though this pattern focuses on a specific kind of policy document. Were this pattern to have a broader context, it could even be considered an accessible policies pattern\u00a0itself.H. Baraki et al., Towards Interdisciplinary Design Patterns for Ubiquitous Computing Applications. Kassel, Germany,\u00a02014.J. Tidwell, Designing interfaces. O\u2019Reilly Media, Inc.,\u00a02005.", "tags": "", "title": "Abridged Terms and Conditions"},{"url": "/patterns/Personal-Data-Table", "text": "Controllers which maintain software systems that process user data, especially identifying or sensitive data, are subject to various laws. In the case of personal data, transparency about processing is particularly important. Users (the data subjects) also care to know about what data is used, and what might be done with that data, at various degrees. Users do not often want to be constantly notified or reminded, as many of them would rather spend their time actually using the system. Some users, however, care about more intricate detail, and are entitled to it. Nonetheless, if verbose information is provided, it should be\u00a0sensible.The controller wants to be upfront about what they know and can do with personal data which might be of importance to those users. They only want users to know about data and risks pertaining to them\u00a0specifically.Keep track of the processing that occurs on personal data so that users can view the activities associated with their data and review their preferences in a tabular\u00a0environment.Provide users with access to an interface which displays their data in useful dataset views, and give them the option for raw information. See the following table for an\u00a0example.|Type of Data|Data|Date Recorded|Accessed by|\n|--|--|--|--|\n|data type a|data itself|date a|person one|\n|data type b|data itself|date b|person one, person\u00a0two|To be really transparent, also show things like how and why data was used, who of your organization has access to the user\u2019s personal data, what was downloaded or sent to a specific third party, and when all these events happened. The table can present all the data at once, or order it in categories, that may be further detailed when the user selects a\u00a0category.Figure 1 shows the actual design of the personal data table pattern implementation for a Quantified Self data store, the Nutritional Research Cohort (NRC). The NRC is a cohort of researchers in nutrition and health sciences who gather self-assessment data on their lifestyle and their health. NRC gives access to information on personal health trajectory, and the effects of diet on personal health. For each column, a mouse overlay details the meaning of the column name. This solution implements an overview of which data is collected, whether data is private or shared with others, for which purpose the data is used, which external parties requested the data, and who downloaded the data and when. This overview is shown on a special page in a myData section of the NRC\u00a0application.Patterns that also show personal data within a user application are the Personal Data Infographic (showing data as infographic, not a table) and Viewable Personal Data Overlay (showing in an overlay which data is viewable by others)[.] Furthermore, the Digital File with Personal Data pattern allows a user to receive the personal data collected for a certain service in a digital\u00a0file.J. Siljee, \u201cPrivacy transparency patterns,\u201d in EuroPLoP \u201915, 2015, pp. 1\u201311.\nW. Kraaij, B. Hulsebosch, J. Siljee, M. Kooij, R. Kosman, and J. Janssen, \u201cPrivacy Tansparency,\u201d SWELL, 2014. [Online]. Available: http://www.swell-project.net/dynamics/modules/SFIL0100/D4.9%20Privacy%20Transparency893d.pdf. [Accessed:\u00a010-Oct-2017].", "tags": "", "title": "Personal Data Table"},{"url": "/patterns/Reciprocity", "text": "Fair distribution of efforts / Win-win\u00a0situationIn services where users may either socially or collaboratively contribute, participation may be a foundation for the service's business model. In these situations the quality and frequency of content affects the success of the service, and thus users have a large impact on its survival. Whether any single user contributes, or not, plays a role in profitability, which puts the controller in a position to encourage or enforce equal participation. Users may respond to such ideas negatively, however, especially if they do not see potential gains worthy of their effort and personal risks to\u00a0privacy.Equal participation does not always result in equal rewards. In some cases, participants do not need to contribute at all to benefit from the content generated by the group. Any who feel slighted are then likely to contribute less, eventually jeopardizing results for the\u00a0group.Limit the benefits gained from the group effort to the amount of effort contributed. All contribution should be afforded proportionate\u00a0gains.Ensure that all group members' activities result in an improved group result that is beneficial for all group members again. Prohibit people to benefit from group results if they are not willing to help the group in\u00a0return.Prior to completing designs on functionality, determine the benefits as opposed to efforts or costs on all possible user activities. Weigh these, with input from any necessary stakeholders. Any feature which does not affect more than one user does not need to be\u00a0assessed.For user groups that are able to affect one another within a feature or functionality, consider them each a case for a collaboration mode. If a user within this group performs an activity, they are expected to reciprocate on any benefits (or gain from costs). This is in proportion to the weighted effort of the feature determined\u00a0earlier.The way in which users reciprocate is up to specific implementation. It may include required effort (satisfied by certain activities) before their activity's resulting benefit is realized. Alternatively it may prevent additional beneficial activities until they contribute. It may also make their discrepancy public, allowing the users to determine tolerable thresholds. In all these cases it is useful to keep track of each user's ratio within each collaboration mode they\u00a0feature.It is important that any use of user data is done so under the explicit and properly obtained permissions required. Deriving value from participation rewards users for providing personal information, and thus they must be informed about how their data may be\u00a0used.As per Sch\u00fcmmer (2004), it is also complemented by the following patterns which are invasive by design, but introduce useful notions:\n- Show the Expert (dark pattern), showing contributions prominently;\n- Who\u2019s Listening (dark pattern), feedback by\u00a0subscribers.Based\u00a0on:T. Sch\u00fcmmer, \u201cThe Public Privacy \u2013 Patterns for Filtering Personal Information in Collaborative Systems,\u201d in Proceedings of CHI workshop on Human-Computer-Human-Interaction Patterns,\u00a02004.", "tags": "", "title": "Reciprocity"},{"url": "/patterns/Ambient-notice", "text": "Instant User Interface for Information (about PII) / Talking Collector / On the Spot\u00a0InformationModern sensor systems process massive quantities of data, performing complex and silent operations which users typically overlook. The tracking of user information is used to improve the quality of these services (or products), and typically users wish to benefit from this. This is particularly evident in consumption, location, and physical activity tracking. While these users do not want to be exposed to extensive and otherwise intimidating details, this processing must be done under the user's informed and explicit consent. Once the consent for it has been obtained,  processing may occur regularly or in real-time. While users are to be informed of this, in-progress readings still happen in a manner which is streamlined and not inherently noticeable. Users are also capable of forgetting some time after consent was\u00a0given.Users are frequently unaware of the sensors currently tracking them. It is important that they understand that their personal data is being further collected in order for their informed consent to remain valid. This should be unobtrusive, however, so as to avoid notification fatigue or\u00a0desensitization.A user may not realize that an application given permission to access [sensor data] is doing so continuously or repeatedly, or may not remember that explicit permissions given in the past allow a service to access data again later. In some cases, past explicit permission may not have been provided by the current user of the device (but instead by a spouse, parent or even an ex-spouse or stalker who temporarily had control of the device or the account). If notice is provided only at the time of consent, a user may inadvertently distribute personal information over a long period of time after having lost control of their device only\u00a0momentarily.A tray full of ambient notices may annoy or confuse users and inure them to ongoing practices. Take measures to avoid unnecessary notice. This must be balanced against the\nconcerns of an attacker's opting the user in without their\u00a0knowledge.Provide an unobtrusive but clearly visible notification while sensors are in use, without interrupting the flow of user activity. This notification should be interactive in order to prevent, delay, or further explain the data\u00a0collection.The best place to provide transparency is the place where data is collected. The sensor is the first component noticed of a complex system, because the user is directly confronted with the sensor during collection. Hence, provided information in this place is easier to access by the user. Because of that, sensors should be equipped with a user interface for instant check of the collected data. Such an interface can consist of a simple display or message box. In more complex environments, optical codes and links can refer the user to more elaborated information\u00a0sources.Mac OS X Lion adds an ambient location services icon (a compass arrow) which appears in the task bar momentarily when an application is accessing the device's\u00a0location.Chrome adds a cross-hair icon to the location bar when a web site accesses the device location via the W3C Geolocation API. Clicking on the icon provides potential actions: clearing the saved consent for this site and accessing\u00a0settings.Similar examples exist in at least Android, iOS and\u00a0Windows.QR-code based information access, smart meter\u00a0displayN. Doty, M. Gupta, and J. Zych, \u201cprivacypatterns.org - Privacy Patterns,\u201d privacypatterns.org, 2017. [Online]. Available: http://privacypatterns.org/. [Accessed:\u00a026-Feb-2015].C. Bier and E. Krempel, \u201cCommon Privacy Patterns in Video Surveillance and Smart Energy,\u201d in ICCCT-2012, 2012, pp.\u00a0610\u2013615.J. Siljee, \u201cPrivacy transparency patterns,\u201d in EuroPLoP \u201915, 2015, pp.\u00a01\u201311.", "tags": "", "title": "Ambient Notice"},{"url": "/patterns/Informed-Implicit-Consent", "text": "Permission to Use Non-sensitive Personal Data - Implicit\u00a0ConsentProcessing of user (data subject) information, particularly that which potentially identifies a user or group, requires their explicit informed consent. Inaction is not considered valid consent. However, not all instances make this feasible. As such there are circumstances in which legitimate interests of the controller may justify collection without first obtaining a clear statement of permission to do so. Security footage around a controller's premises, or fraud detection, for example, cannot reasonably be made optional to users of the service (or product). What constitutes legitimate interests in these contexts depends on the relationship and reasonable expectations between the controller and user. As such, sensitive data, or special categories of data, are more difficult to\u00a0justify.A controller needs to collect and otherwise process reasonable information to fulfill their legitimate interests regarding a user, but cannot feasibly acquire each user's explicit\u00a0consent.Provide clear and concise notice that by using the service, the user implicitly consents to the processing necessary to fulfill legitimate interests. Ensure that this notice is perceived prior to the application of the effects it\u00a0describes.Ensure that users are informed sufficiently prior to any processing with clear and concise notice, the complete detail of which should also be accessible. In digital mediums, this is straightforward, working similarly to Cookie Walls on websites. Users should be given the opportunity to choose not to use the service and therefore not be subject to the processing it\u00a0requires.In physical instances it is more difficult to be sure that users take note of this. On devices, lights have often been used to convey a recording state. This, while clear once already subject to processing, is not sufficient however. Instead, large signs are commonplace to indicate the use of data collection. The most familiar example would be \"Smile, you're on camera\". Of course, this is less clear than \"Our premises is recorded for security purposes, by entering you consent to this processing. See more info at [address]\". These signs should be posted, visible prior to recording, at all entrances or otherwise where\u00a0applicable.Users will be informed before implicitly providing consent to reasonable processing for legitimate interests of the\u00a0controller.Given a Sensor Network, Provider, and Controller, collected data is delegated by the Controller through the Provider to the Sensor Network. The Sensor Network collects some data with explicit consent, but this data may also be personal for a user who has not given such consent. This data may be potentially identifying, and thus the user should be informed prior to its processing. The Controller must ensure that the Provider of the Sensor Network provides any potential users with unambiguous warning of the collection, and that individual consent is infeasible. This may make use of a clear and legible warning sign. The Sensor Network itself should also be visible and obvious, clearly indicating when collection is ongoing. Societal norms may dictate this, such as security cameras in some contexts (commercial areas where valuables may be stolen) needing little\u00a0warning.Y. Asnar, V. Bryl, R. Bonato, L. Campagna, D. Donnan, P. El Khoury, P. Giorgini, S. Holtmanns, M. Martinez-Juste, F. Massacci, J. Porekar, C. Riccucci, A. Saidane, M. Seguran, R. Thomas, and A. Yautsiukhin, \u201cInitial Set of Security and Privacy Patterns at Organizational Level,\u201d\u00a02007.", "tags": "", "title": "Informed Implicit Consent"},{"url": "/patterns/Aggregation-gateway", "text": "Encrypt, aggregate and decrypt at different\u00a0places.A service provider gets continuous measurements of a service attribute linked to a set of individual service\u00a0users.The provision of a service may require detailed measurements of a service attribute linked to a data subject to adapt the service operation at each moment according to the demand load. However, these measurements may reveal further information (e.g. personal habits, etc.) when repeated over\u00a0time.A feeder metering system can be added as a measuring rod which introduces a comparison for each group of\u00a0meters.Let the service provider have reliable access to the aggregated load at every moment, so as to fulfil its operating requirements, without letting it access the individual load required from each specific service\u00a0user.There is a need to deploy trusted third parties that compute the aggregations over each group of users. Note that they need to be honest (i.e., they cannot collude with the other parties involved), but they need not respect confidentiality (as they only have access to encrypted contents). Smart meters are needed that have computation resources to apply secret generation and homomorphic encryption procedures (note that this is trivial when dealing with the use of computational resources, but it does not have to be always available in the case of e.g. smart grid systems). The potential range of measured values must be large enough to avoid brute force attacks. Robust homomorphic encryption schemes introduce a large computational\u00a0load.An electric utility operates a smart grid network with smart meters that provide measurements of the instantaneous power consumption of each user. The utility employs that information to adapt the power distribution in a dynamic fashion, according to the user demand at each\u00a0moment.", "tags": "", "title": "Aggregation Gateway"},{"url": "/patterns/Pseudonymous-messaging", "text": "A messaging service is enhanced by using a trusted third party to\nexchange the identifiers of the communication partners by\u00a0pseudonyms.This pattern can be used for online communications by email, through\nmessage boards, and\u00a0newsgroups.Messaging includes all forms of communication through emails,\narticles, message boards, newsgroups etc. This information could be\nstored and used to build sophisticated user profiles. Sometimes it can\nalso be used to prosecute\u00a0people.A message is send by a user to the server, which exchanges the\nsender's address with a pseudonym. Replied messages are sent back to\nthe pseudonymous address, which will then be swapped back to the\u00a0original.The goal of this pattern is to prevent unforeseen ramifications of the\nuse of online messaging\u00a0services.Users can communicate more freely. Pseudonym servers can be misused to\nsend offensive messages, for spam mails or by criminals for illegal\nactivities. Under those circumstances it could be necessary to revoke\nthe pseudonymity of the corresponding\u00a0parties.Alice is a political activist and tries to organize a political\ndemonstration. Since her government does not like free speech, her\ncommunication channels are intensely monitored and one day, she simply\ndisappears into a labor camp and is never seen\u00a0again.nym.alias.net a pseudonymous email system with the goal to provide\nsecure concealment of the user's identity. A Type I Anonymous Remailer\nforwards emails by modifying the message header and removing\nsender-related\u00a0information.", "tags": "", "title": "Pseudonymous Messaging"},{"url": "/patterns/Attribute-based-credentials", "text": "Attribute Based Credentials (ABC) are a form of authentication mechanism that allows to flexibly and selectively authenticate different attributes about an entity without revealing additional information about the entity (zero-knowledge\u00a0property).ABC can be used in a variety of systems including Internet and smart\u00a0cards.Authentication of attributes classically requires full and unique authentication of an entity. For example, attributes (like age) could be put into a certificate together with name of the user, email address, public key, and other data about that entity. To corroborate an attribute (for example, that the user is an adult) the certificate has to be presented and all information have to be revealed. This is not considered a privacy-preserving\u00a0solution.There are multiple schemes to realize ABCs and implementations are also available. They typically all include a managing entity that entitles issuers to issue credentials to entities that could then act as provers of certain facts about the credentials towards\u00a0verifiers.A formal model can be found\u00a0here.To allow a user to selectively prove specific attributes like age > 18 to a verifying party without revealing any additional\u00a0information.ABC schemes require substantial compute power or optimization, so implementation may not be straightforward. Some projects like IRMA developed at Radboud University Nijmegen have shown that even resource restricted devices like smartcards can implement\u00a0ABCs.You want to issue an ID card that holds a users birthdate bd and can be used to prove that the card holder is old enough to view age-restricted movies in a cinema. Depending on the rating of the movie (minimum age x), the card holder can run a proof\u00a0that:Multiple uses of the card at the same cinema should not be\u00a0linkable.The most popular implementations\u00a0include:", "tags": "", "title": "Attribute Based Credentials"},{"url": "/patterns/Privacy-Labels", "text": "Privacy Nutrition\u00a0LabelsUsers use a variety of services (or products) for which there are different effects on their privacy. The providers of these services have varying policies around that usage, and thus affect privacy differently. Typically the differences appear in a privacy policy document, or set of documents. Services encourage users to read this information, which can be quite extensive and involved. Users do not typically have the time or patience to investigate this information on their\u00a0own.Due to the effort required, users often do not investigate the various privacy policies of the services they use, leaving them uninformed about the potential consequences of their consent and choices. Services tend to have overly complex policies, and present them inconsistently, which agitates this\u00a0issue.Present the user with an standardized privacy 'nutritional' label to quickly summarize policy\u00a0information._Putting a box around the label identifies the boundaries of the information, and, importantly, defines the areas that are \u201cregulated\u201d or should be trusted. This is a common issue when the label is placed in close proximity to other information, but may not be as significant an issue online.\u00a0_Using bold rules to separate sets of information gives the reader an easy roadmap through the label and clearly designates sections that can be grouped by\u00a0similarityProviding a clear and boldfaced title, e.g., Privacy Facts, communicates the content and purpose of the label specifically and assists in\u00a0recognition.Finally, we have defined a maximum width of 760px for this label and all following designs in this paper. One important consideration was that the privacy label design be printable to a single page and viewable in the standard width of today\u2019s internet\u00a0browsers.P.G. Kelley, L.J. Cesca, J. Bresee, and L.F. Cranor. Standardizing Privacy Notices: An Online Study of the Nutrition Label Approach. CHI\u00a02010.P. Kelley, J. Bresee, L. Cranor, and R. Reeder. A \"Nutrition Label\" for Privacy. SOUPS\u00a02009Kleimann Communication Group, Inc. Evolution of a Prototype Financial Privacy Notice. February 2006. Available:\u00a0http://www.ftc.gov/privacy/privacyinitiatives/ftcfinalreport060228.pdfReeder, R.W. Expandable Grids: A user interface visualization technique and a policy semantics to support fast, accurate security and privacy policy authoring. PhD thesis, Carnegie Mellon. 2008.\u00a0http://www.robreeder.com/pubs/ReederThesis.pdf", "tags": "", "title": "Privacy Labels"},{"url": "/patterns/Buddy-List", "text": "Contact List, Address\u00a0BookUsers frequently interact upon various media, forums, and communication channels. There are however far more users on these channels than most would be comfortable wading through. As controllers for such channels, many services wish to aid their users in finding familiar and comfortable interactions. Users may also seek to participate outside their immediate circles, but may aim not to stray too\u00a0far.When many users are able to interact in the interaction space, it is hard to maintain an overview of relevant interaction partners since the number of users exceeds the number of relevant contacts for a specific user. User lists grow very large and it is hard to find people who the local user knows. On the other hand, the local user is [more interested in close\u00a0contacts].A service aims to provide users with shortcuts to interaction with users who they are most likely to interact with within a particular context (close contacts within social\u00a0circles).Allow users to find and assign others to a user-maintained directory of social circles and contexts to interact with. This is optionally only visible to the users\u00a0themselves.Users should be able to view the Buddy List on demand, either during a search operation or persistently. They should be able to add or remove users from the relevant list with minimal\u00a0effort.The list may be seen as a set of user objects. This buddy list has the possibility of adding or removing user objects. In the first case, whenever the local users interact with another user, they can add the other user to their buddy list. To reach this goal, in the user interface, the local users can select the representation of the another user and execute a command for adding (e.g. a menu item associated to the user object). For removing users, when the buddy list is shown, the local users can select the representation of the another user and execute a command for removing (e.g. a menu item associated to the user\u00a0object).The Buddy List may fuse with other common interaction idioms to constitute a more comprehensive approach to the problem, making it more than an\u00a0idiom.Connecting the means for adding users to the buddy list with the user\u2019s representation (or the interface elements that are used to interact with the other user) makes the process of adding a user to the buddy list intuitive and reminds a user to consider adding the\u00a0user.By using the Buddy List to make connections about the user, the service can recommend relevant contact\u00a0suggestions.If users only consider buddy lists for maintaining contacts to other users, they will hardly find new users in the system. Thus you should ensure that users can also browse other users who are not on their buddy list (e.g. by providing a User\u00a0Gallery).The service can trivially derive the social structure of its userbase which may put trust at\u00a0jeopardy.Based\u00a0on:T. Sh\u00fcmmer, \u201cThe Public Privacy \u2013 Patterns for Filtering Personal Information in Collaborative Systems,\u201d in Proceedings of CHI workshop on Human-Computer-Human-Interaction Patterns,\u00a02004.", "tags": "", "title": "Buddy List"},{"url": "/patterns/Incentivized-Participation", "text": "Reciprocity / Fair distribution of efforts / Win-win situation /\u00a0Pay-BackUsers of a system have varying privacy concerns, and different sensitivities associated with their personal information. These users need ways to contribute without leaking sensitive details, or to perceive a worthwhile tradeoff for those details. This can be achieved through social encouragement (i.e. participation and shared trust), direct value exchanges (discounts and giveaways), or some other derived value (e.g. positive\u00a0reinforcement).A data controller derives various values from the participation of its users (i.e. data subjects). The more that these users participate, explicitly providing context and implicitly providing metadata (e.g. statistics and telemetry), the better the controller fares in a number of respects. Despite this key relation, over-sharing can greatly infringe upon a user's right to privacy. Many controllers therefore aim to respect this right when benefiting from user\u00a0interactions.Controllers which gain from user activity want to push for participation, but this can negatively affect\u00a0users.Users have varying degrees of concern about their privacy, and do not respond to different forms of encouragement the same way. By penalising under-sharing and inactivity, or being misleading, users become alienated and distrusting of the system. As such this problem has multiple elements. These include asymmetric returns on investment, and the standard incentive deficiency, where users lack the encouragement to\u00a0participate.In many situations, some benefit more than others. In extreme cases, users may benefit through minimal participation and thus contribute very little to the system's derived value. Those who do not perceive an acceptable value despite considerable contribution may then\u00a0withdraw.An example of this behaviour might be seen in dating sites where users with only a flattering picture may succeed more than those with detailed profiles. Similar cases can be made for other social media, as well as with asymmetric bandwidth on peer to peer sharing. With torrent technologies, this is often referred to as\u00a0'leeching'.Users which provide limited or vague information due to privacy concerns may have less opportunity for participation. Another way this occurs is when they are not driven by positive social reinforcement. The lack of friends, followers, potential matches, etc. leads to user\u00a0inactivity.Privacy concerns need to be met with valid reassurances about issues which matter to the user. Firstly, users should know that the system holds their preferences in high regard. Secondly, they should perceive real value in their participation. Finally, if desired, users should be assisted in a smooth transition into the\u00a0ecosystem.The three elements of the solution are elaborated on in the following\u00a0sections.Users need to know they are able to participate without the system undermining their personal preferences. This should apply from the very first usage of a system. Everything the system does globally must adhere to privacy friendly defaults. Any service which cannot uphold these expectations should be deactivated for new users, and only be enabled once these users consent to the additional processing. Attempts to solicit this should not be\u00a0invasive.With privacy concerns at ease, encouraging equal participation entails reciprocity. These can be in both social and financial\u00a0forms.All participation should result in value derivation (social or otherwise) for all participants, and not just individuals. As a consequence of this mindset, the derived benefits of users who do not participate are limited. This secondary effect may also be the primary mechanism for reciprocity, though positive sum approaches will be met with more\u00a0support.As an additional measure, or where equality is not feasible, provide an alternative incentive. If not social, then financial incentives (discounts, waived fees) can be provided to active users. This can be limited to those who the system can identify as receiving poor value returns on their contributions. User retention examples tend to be less frequent, with the odd website sending 'come back' emails which promise fee\u00a0reductions.Another approach is the explicit provision of virtual currency necessary to benefit from the system, those who contribute will then have more currency at their disposal. They may opt to be applauded for their efforts publicly, but again should not be forced\u00a0to.Examples of social value perception are the Facebook like/reaction, Google's +1, Reddit Gold, and Twitter's reposting. These approaches are enacted by the users instead of the system, and are therefore less\u00a0intrusive.In terms of computer aided pairing, greater participation may be achieved if users consent to intelligent nudging. Sh\u00fcmmer suggests that users are more likely to participate when their interests are shared with others, and thus, a system would help users identify those with similar interests. This is another solution which relies on prior consent, however. By encouraging interaction between these users, a system would derive more activity and therefore further\u00a0value.On the other hand, Sh\u00fcmmer points out that mixing dissimilar users may also result in unexpected activity. It may allow the system to discover notions about its users which were not previously apparent. This is yet another way to increase value, though will likely be far more intrusive than the former. Users should be properly informed of the possible consequences of 'mixing it\u00a0up'.In order to ensure that recommendations made by the system do not have increasingly negative side effects, the system should learn from ineffective suggestions. This is limited to where it has permission to do so. Where user activity drops, a system should aim not continue in the same fashion as before. When it climbs, however, the system should improve whatever characteristics likely resulted in that\u00a0climb.This can be made more explicit by soliciting feedback from the user themselves, as also suggested by Sh\u00fcmmer. Even this, though, is subject to negative reactions. However, acclimating users to an environment of openness and transparency will also build trust - potentially resulting in the use of services users would not have used\u00a0otherwise.Applying the concepts represented in this pattern may have certain trade-offs\u00a0associated.Isolating users, and learning from their actions, based on feedback loops requires prior informed and explicit consent, as potentially invasive conclusions may be\u00a0derivedPermission restricted Buddy Lists in Instant Messaging, or more extensive social networks; Allowing users to filter their sharing by access groups which they\u00a0define.Examples of Value\u00a0PerceptionFacebook like/reaction, Google's +1, Reddit Gold, and Twitter's\u00a0repostingExamples of Transition\u00a0AssistanceT. Sh\u00fcmmer, \u201cThe Public Privacy \u2013 Patterns for Filtering Personal Information in Collaborative Systems,\u201d in Proceedings of CHI workshop on HCHI Patterns,\u00a02004.", "tags": "", "title": "Incentivized Participation"},{"url": "/patterns/Anonymous-reputation-based-blacklisting", "text": "Get rid of troublemakers without even knowing who they\u00a0are.A service provider provides a service to users who access anonymously, and who may make bad use of the\u00a0service.Anonymity is a desirable property from the perspective of privacy. However, anonymity may foster misbehaviour, as users lack any fear of\u00a0retribution.A service provider can assign a reputation score to its users, based on their interactions with the service. Those who misbehave earn a bad reputation, and they are eventually added to a black list and banned from using the service anymore. However, these scoring systems traditionally require the user identity to be disclosed and linked to their reputation score, hence they conflict with anonymity. This has made, for instance, Wikipedia administrators to take the decision to ban edition requests coming from the TOR network, , as they cannot properly identify users who\u00a0misbehave.A Trusted Third Party (TTP) might be introduced in between the user and the service provider. The TTP can receive reputation scores from the service provider so as to enforce reputation-based access policies, while keeping the identity hidden from the service provider. However, this would require the user to trust the TTP not to be a whistle-blower\u00a0indeed.How can we make users accountable for their actions while keeping them\u00a0anonymous?First, the service provider provides their users with credentials for anonymous\u00a0authentication.Then, every time an authenticated user holds a session at the service, the service provider assigns and records a reputation value for that session, depending on the user behaviour during the session. Note that these reputation values can only be linked to a specific session, but not to a specific user (as they have authenticated\u00a0anonymously).When the user comes back and starts a new session at the service, the service provider challenges the user to prove in zero-knowledge that he is not linked to any of the offending sessions (those that have a negative reputation associated). Zero-knowledge proofs allow the user to prove this, without revealing their identity to the service provider. Different, alternative proofs have been proposed, e.g. prove that the user is not linked to any of the sessions in a set of session IDs, prove that the last K sessions of the user have good reputation,\u00a0etc.In practice, more complex blacklisting rules can be applied as well. For instance, several reputation scores can be assigned to the same session, each regarding different facets of the user behaviour. Then, the blacklisting thresholds may take the form of a Boolean combination or a lineal combination over individual session and facet reputation\u00a0values.A service provider wants to prevent users who misbehave from accessing the service anymore, without gaining access to their\u00a0identity.Different implementations may only be practical for services with a reduce number of users, require intense computations, limit the scope of the reputation to a constrained time frame, be vulnerable to Sybil attacks, etc. Nonetheless, protocols are being improved to overcome these and other issues. See the cited sources below for the specific\u00a0discussion.A wiki allows any visitor to modify its contents, even without having been authenticated. Some malicious visitors may vandalize the contents. This fact is signalled by the wiki administrators. If a visitor coming from the same IP address keeps vandalizing the site, they will earn a bad reputation, and their IP will be banned from modifying the contents anymore. However, users accessing through a Tor anonymity network proxy cannot be identified from their IPs, and thus their reputation cannot be\u00a0tracked.", "tags": "", "title": "Anonymous Reputation-based Blacklisting"},{"url": "/patterns/Pay-Back", "text": "In services where users may contribute content, or provide the system with account or profile information, the information is only valuable if relevant and accurate. For controllers providing this service (or product), worthless information does not typically generate income or future participation. Without consistent usage, a service becomes less popular and eventually may run at loss. This is particularly true in socially oriented services. To keep the service working, it is crucial that its users maintain content. Users however, might not feel inclined to do so. Keeping content up to date, or adding it in the first place, requires effort, and in some cases an acceptance of privacy\u00a0risk.Users do not necessarily want to provide and maintain content, they need a motivation to do so. Without this, a service will not\u00a0flourish.Provide users with different kinds of benefits when they contribute or maintain content for the service and make sure they do so\u00a0consensually.Depending on the kind of service that is provided, different benefits could be considered: virtual or real currency, use of services, social benefits, and so\u00a0on.When using virtual or real currency, the controller should first define how much in value users would receive depending on the contributions. In the case of virtual currency, the places where the currency could be used should be\u00a0defined.Regarding use of service, some criteria could be considered non-exhaustively: feedback on content, frequency of contributions, the use of service for a minimum duration, access to a service earlier than others, or use of special features within the\u00a0service.When users reach a limit, they could additionally assist with virtual or physical events for learning, meeting people, etc. In virtual scenarios, users could receive attention (feedback) from one\u00a0another.Based\u00a0on:T. Sch\u00fcmmer, \u201cThe Public Privacy \u2013 Patterns for Filtering Personal Information in Collaborative Systems,\u201d in Proceedings of CHI workshop on Human-Computer-Human-Interaction Patterns,\u00a02004.", "tags": "", "title": "Pay Back"},{"url": "/patterns/Federated-privacy-impact-assessment", "text": "The impact of personal information in a federation is more than the\nimpact in the\u00a0federatedIdentity Management scenarios (that is, when the roles of the Identity\nProvider and the Service Provider are\u00a0separated).Identity Management solutions were introduced to decouple the\nfunctions related to authentication, authorization, and management of\nuser attributes, on the one hand, and service provision on the other\nhand. Federated Identity Management allows storing a data subject's\nidentity across different systems. All together, these form a\nFederation that involves complex data\u00a0flows.Federated Management solutions can be used to improve privacy (e.g. by\nallowing service providers to offer their services without knowing the\nidentity of their users). However, the complexity of data flows and\nthe possibility of collusion between different parties entail new\nrisks and threats regarding personal\u00a0data.A Privacy Impact Assessment is conducted by all the members of the\nfederation, both individually and in conjunction, so as to define\nshared privacy policies, prove they are met, and demonstrate the\nsuitability of the architecture, in the benefit of all the\u00a0members.Deal with privacy risks associated from the federation of different\nparties in an Identity Management\u00a0solution.The consequences depend on the results of the privacy-impact\u00a0analysis.An Identity Provider issues pseudonyms to authenticate users at\nthird-party Service Providers, which can in turn check the\nauthenticity of these pseudonyms at the Identity Provider, without\ngetting to know the real user identity. However, the Identity Provider\nknows all the services requested by the users, which discloses\npersonal information to the Identity Provider and allows it to profile\nthe\u00a0users.", "tags": "", "title": "Federated Privacy Impact Assessment"},{"url": "/patterns/Masquerade", "text": "Anonymous\u00a0InteractionUsers act differently under active supervision, and this may negatively impact their content\u00a0generation.Allow users to select their desired identifiability for the context in question. They may reveal some subset of the interaction or account attributes and filter out the\u00a0rest.For implementing this pattern, a configuration interface will be required. Two approaches could be considered: levels of publicity or publicity\u00a0profiles.In levels of publicity, all possibly revealed information could be arranged on a scale depending on how identifying each kind of information is alone or when shown together. A visual element could be used to select a specific publicity level. When the users select one level, all information with the same or smaller publicity level will be revealed. This is taken into account when measuring where upon the scale a piece of information\u00a0falls.In publicity profiles, all possibly revealed information could be depicted using visual elements and the users have to select each kind of information that they want to reveal. Furthermore, depending on the kind of information, the users could define different granularity for each one (E.g. regarding location it is possible to define the country, region, city, department and so\u00a0on).Based\u00a0on:T. Sch\u00fcmmer, \u201cThe Public Privacy \u2013 Patterns for Filtering Personal Information in Collaborative Systems,\u201d in Proceedings of CHI workshop on Human-Computer-Human-Interaction Patterns,\u00a02004.", "tags": "", "title": "Masquerade"},{"url": "/patterns/Onion-routing", "text": "This pattern provides unlinkability between senders and receivers by\nencapsulating the data in different layers of encryption, limiting the\nknowledge of each node along the delivery\u00a0path.A system in which data is routed between different\u00a0nodes.When delivering data, the receiver has to be known. If the system\nprovides the functionality that the receiver of data should be able to\nanswer, than the receiver should also know the address of the sender.\nWhen forwarding information over multiple stations then, in a naive\nimplementation, each station on the delivery path knows the sender and\nthe final\u00a0destination.The solution is to encrypt the data in layers such that every station\non the way can remove one layer of encryption and thus get to know the\nimmediate next station. This way, every party on the path from the\nsender to the receiver only gets to know the immediate successor and\npredecessor on the delivery\u00a0path.The goal of this pattern is to achieve unlinkability between senders\nand\u00a0receivers.If there are too few hops, the anonymity set is not big enough and the\nunlinkability between sender and receiver is at risk. The same problem\noccurs when there is too few communication going on in the network.\nThe multiple layers of encryption will bloat up the data and consume\nbandwidth. If all nodes on the delivery path collaborate in deducing\nthe sender and the receiver, the pattern becomes\u00a0useless.Alice is a whistle-blower and tries to forward data to Bob who works at\nthe press. She sends the corresponding documents as an\ne-mail-attachment. Eve monitors the traffic and can see who sent this\nmail to whom. The next day, police raids Alice's apartment and sends\nher to jail. Bobs mail account gets\u00a0seized.The TOR-browser, a web-browser specifically designed to ensure\nanonymity makes heavy use of onion\u00a0routing.", "tags": "", "title": "Onion Routing"},{"url": "/patterns/Obligation-management", "text": "The pattern allows obligations relating to data sharing, storing and\nprocessing to be transferred and managed when the data is shared\nbetween multiple\u00a0parties.The developer aims to make sure that multiple parties are aware of and\ncomply with required user/organisational policies as personal and\nsensitive data are successively shared between a series of parties who\nstore or process that\u00a0data.Data may be accessed or handled by multiple parties that share data\nwith an organisation in ways that may not be approved by the data\u00a0subject.Service providers use an obligation management system. Obligation\nmanagement handles information lifecycle management based on\nindividual preferences and organisational policies. The obligation\nmanagement system manipulates data over time, ensuring data\nminimization, deletion and notifications to data\u00a0subjects.Benefits: privacy preferences and policies are communicated and adhered\nto among organisations sharing data. Liabilities: additional effort to\nset\u00a0obligations.A service provider subcontracts services, but requires that the data\nto be deleted after a certain time and that the service provider\nrequires to be notified if there is further\u00a0subcontracting.Pretschner et al (2009) provide a framework for evaluating whether a\nsupplier is meeting customer data protection obligations in\ndistributed systems. Researchers at IBM propose Enterprise Privacy\nAuthorization Language (EPAL) (2004) to govern data handling practices\naccording to fine-grained access control. Casassa Mont (2004) discusses\nvarious important aspects and technical approaches to deal with\nprivacy obligations. Pretschner, A., Schtz, F., Schaefer, C., and\nWalter, T.: Policy Evolution in Distributed Usage Control. Electron.\nNotes Theor. Comput. Sci. 244, 2009 IBM, The Enterprise Privacy\nAuthorization Language (EPAL), EPAL specification,\nhttp://www.zurich.ibm.com/security/enterprise-privacy/epal/, 2004\nMont, M. C., Dealing with Privacy Obligations, Important Aspects and\nTechnical Approaches, TrustBus,\u00a02004", "tags": "", "title": "Obligation Management"},{"url": "/patterns/Trust-Evaluation-of-Services-Sides", "text": "Privacy Transparency\u00a0LabelWhen using a service (or product) offered by a controller, the level of trust held by users is crucial. Without sufficient trust, the users would seek alternatives or generate bad publicity. They will use a system more cautiously, regardless of whether it is necessary. In many systems this lessens the quality of service offered, not only to the user in question, but\u00a0holistically.Users want to have reason to trust that a service does not undermine their personal privacy requirements. They do not want to have to take controllers, and third parties, at their word\u00a0alone.Supply a function which informs users of the trustworthiness and reliability of services, and that of the third parties connected to those services. These qualities may be determined, and assured, through independent evaluation of given\u00a0criteria.Information regarding a service's trustworthiness and reliability needs to be clearly indicated to the user prior to or during collection. It may therefore be brought up along with obtaining informed consent. This ensures that the user does not make misinformed or uninformed decisions, especially as this can seriously jeopardise\u00a0trust.A visual highlight which succinctly asserts this quality may also be displayed in persistent manner, or where otherwise contextually\u00a0relevant.A trust evaluation function should be based on suitable parameters for measuring the trustworthiness of communication partners and for establishing reliable\u00a0trust.[Trust] in a service provider can be established by monitoring and enforcing institutions, such as data protection commissioners, consumer organisations and certification bodies. Privacy seals certified by data protection commissioners or independent certifiers (e.g., the EuroPrise seal, the TRUSTe seal or the ULD G\u00fctesiegel) therefore provide especially suitable information for establishing user trust. Such static seals can be complemented by dynamic seals conveying assurance information about the current security state of the system and its implemented privacy and security (PrimeLife) functions. Further information sources by independent trustworthy monitoring organisations that can measure the trustworthiness of services sides can be blacklists maintained by consumer organisations or privacy alert lists provided by data protection\u00a0commissioners.[Also,] reputation metrics based on other users' [ratings] can influence user trust. Reputation systems, [for instance] in eBay, can however often be manipulated by reputation forging or poisoning. Besides, the calculated reputation values are often based on subjective ratings by non-experts, [through which privacy-friendliness may be difficult to\u00a0judge].Users will be able to better justify the trust they place in controllers who measure high levels of trustworthiness and reliability, and will know of greater risks in lower trust. A familiarity with the approach will also cause a healthy skepticism of controllers who do not participate, or have low confidence\u00a0evaluations.Determine an appropriate metric for evaluating trustworthiness of partners of the service who will receive personal data as third parties. This can be simple, such as meeting expectations, failing them, or exceeding them. PrimeLife suggests 'poor', 'fair', and 'good', with fair evaluations having neither negative nor positive influences. Blacklists or alert lists make for a poor evaluation regardless of positive\u00a0aspects.These evaluations are shown to users prior to their related parties having consent for access. The notification is not shown too frequently, as extensive warnings may be misleading to users. While they should be aware of neutral or unevaluated parties, it may not be desired to worry them without cause. There should also be just enough information to raise awareness, allowing the user to investigate further if desired. A notification for a fair evaluation may be 'we have not found any issues with this partner' for example, with a neutral colour which matches the rest of the interface. Poor evaluations could be yellow or red (alarming colours), with good evaluations green or blue (positive\u00a0colours).Additional information can be found in the original version of the pattern:\nS. Fischer-H\u00fcbner, C. K\u00f6ffel, J.-S. Pettersson, P. Wolkerstorfer, C. Graf, L. E. Holtz, U. K\u00f6nig, H. Hedbom, and B. Kellermann, \u201cHCI Pattern Collection - Version 2,\u201d\u00a02010.This pattern is also highlighted in other works:\nJ. Siljee, \u201cPrivacy transparency patterns,\u201d in EuroPLoP \u201915, 2015, pp.\u00a01\u201311.O. Drozd, \u201cprivacypatterns.wu.ac.at - Privacy Patterns Catalog,\u201d privacypatterns.wu.ac.at, 2016. [Online]. Available: http://privacypatterns.wu.ac.at:8080/catalog/. [Accessed:\u00a025-Jan-2017].C. Andersson, J. Camenisch, S. Crane, S. Fischer-H\u00fcbner, R. Leenes, S. Pearson, J. S. Pettersson and D. Sommer. \u201cTrust in PRIME\u201d. Proceedings of the 5th IEEE Int. Symposium on Signal Processing and IT, pages 18-21,\u00a02005.", "tags": "", "title": "Trust Evaluation of Services Sides"},{"url": "/patterns/Impactful-Information-and-Feedback", "text": "Users are frequently in a rush to use services at the same pace as their own ever quickening lifestyles. Such value for time can leave them unaware of the potential for mistakes, such as in automatic media sharing, or the careless disclosure of information in their contributions. These mistakes may disclose personally identifiable information, or otherwise undesirable associations. Sometimes whether the information is appropriate is dependent on the audience, or some other contextual element. Controllers who provide services to these users do not fare well when these instances occur, as they provide the means for it to happen. As such, they tend to want to be proactive in handling such\u00a0issues.A lack of user awareness in the moment can lead to regretted disclosure, whether this disclosure is manually or automatically\u00a0performed.Use contextual privacy warnings, through analytical measures and historical queues to provide relevant information and suggestions regarding pending\u00a0disclosures.Prior to submissions taking place, and provided that the user has consented to contextual privacy warnings, analyze the content of the disclosure using natural language processing. This may also entail additional metadata, such as factors pertaining to the expected audience, social comparisons against similar users or ones which the user in question has connected with. All users from which the analysis is derived should also first have provided their explicit, informed, and freely-given\u00a0consent.Search for strings which are likely to heighten the sensitivity of the content, and evaluate this against the expected audience. Where users disregard warnings, take note for improvement of future predictions through a feedback loop. Additionally, allow users to signify that despite ignoring the warning, they later regretted the post (or detect deletions which imply this) to distinguish false positives from inaccurate\u00a0warnings.One way to increase user understanding of the risks involved is to demonstrate by example a disclosure which matches the approximated sensitivity or contextual appropriateness of their content. This example will need to be one which they could usually view on their own, so as not to inadvertently cross the boundary of another user's privacy. This approach is also susceptible to inaccuracies, and would also need to be improved overall by the\u00a0userbase.The learning algorithm may at first be trained using text mining from logs of users who have opted-in to the, at first, experimental feature. While assumptions may be made, possibly inaccurately, users could also give feedback about regretted submissions or contextual appropriateness. Which type of learning is chosen is dependent on what information the controller has at their disposal at the time. If starting fresh, the implementation will likely be less sophisticated. While if available, solutions can be as complex as a reinforced classification learning\u00a0algorithm.By applying this pattern, users who choose to partake will have a better realization of what might happen when they disclose certain content. This can apply to any information they put online, and may show who will be able to see what. This can be both beneficial and disadvantageous, as this means users will be more cautious and less likely to contribute. They may also have worries about the trustworthiness of the learning algorithm which may access their content before they themselves have seen it fit for\u00a0publication.Systems can reduce user uncertainty about factors important to disclosure choices. For example, systems may be able to estimate the audience for a particular disclosure at decision-time,\nthereby reducing uncertainty and influencing user choices. Systems could use social comparison, such as decisions made by friends or other users in similar context, to reduce uncertainty about relevant norms for disclosure. Finally, tools for viewing photo \u201cdisclosures\u201d in ways similar to how others will view these photos could help users understand the content and appearance of their\u00a0disclosures.Based on:\nS. Ahern, D. Eckles, N. Good, S. King, M. Naaman, and R. Nair, \u201cOver-Exposed ? Privacy Patterns and Considerations in Online and Mobile Photo Sharing,\u201d CHI \u201907, pp. 357\u2013366,\u00a02007.", "tags": "", "title": "Impactful Information and Feedback"},{"url": "/patterns/Privacy-Aware-Wording", "text": "Users are exposed to many privacy policies and notifications which seek to inform them of various issues. The controllers who provide these explanations require that users fully understand the circumstances around the use of their data. Specifically, the purposes for which and means by which their personal data is collected or otherwise processed. There is much information however, and so users are likely to overlook important\u00a0details.Information the controller conveys to the user is frequently overlooked due to length and complexity of both the content and the vocabulary within, which compromises validity of\u00a0consent.Users should clearly understand the content of and terms used within privacy and security software. The terms are usually formulated on an expert-basis and therefore often difficult to understand for the average\u00a0user.Construct privacy related information using easily parsed and low difficultly vocabulary, with short concise sentences and enough flow to persuade the user to process\u00a0it.Before using the terms one should be sure that they are clear and understandable for the target-users. Therefore it is recommended to either refer to standardized terms [or] to conduct user tests on the understandability of [utilized] terms and phrases. These tests do not have to be extensive. Asking only few representative users from the target-group about their understanding of the terms should\u00a0suffice.Referring to the user as the data subject or otherwise introducing terms to the user may reduce reading comprehension. Instead of focusing on legally accurate terms, the information should make sense to the user. It should not be provide a false interpretation, however. The PrimeLife example features a mock corporation which summarises information according to 'what', 'how', and\u00a0'who'.S. Fischer-H\u00fcbner, C. K\u00f6ffel, J.-S. Pettersson, P. Wolkerstorfer, C. Graf, L. E. Holtz, U. K\u00f6nig, H. Hedbom, and B. Kellermann, \u201cHCI Pattern Collection - Version 2,\u201d\u00a02010.C. Graf, P. Wolkerstorfer, A. Geven, and M. Tscheligi, \u201cA Pattern Collection for Privacy Enhancing Technology,\u201d The Second International Conferences of Pervasive Patterns and Applications, vol. 2, no. 1, pp. 72\u201377,\u00a02010.", "tags": "", "title": "Privacy Aware Wording"},{"url": "/patterns/Obtaining-Explicit-Consent", "text": "Explicit Consent / Obtaining Explicit consent via privacy agreement / Permission to Use Sensitive Data via Privacy Agreement - Explicit\u00a0ConsentIn order to offer services (or products) to users (data subjects), controllers often need to collect (process) user data. Sometimes this is sensitive, identifying, or just metadata or other information which may be correlated to become more invasive. This nonetheless enables them to offer competitive features and\u00a0functionality.However, controllers are required to obtain unambiguous consent from their users in order to process their personal data in any way. Depending on the legal jurisdiction, there are additional considerations to take into account depending on the type of data in question. Typically, sensitive data requires especially rigorous\u00a0care.Controllers which aim to make use of user data, especially that which can be used to identify the user or sensitive aspects about the user, may not do so without a legally binding and sound acquisition of the user's\u00a0consent.The controller must ensure each user's sufficient understanding of the potential consequences. Otherwise the consent might not be informed. They must verify their users' willingness despite those consequences to provide their data for the specific purposes they need. If they do not, the consent might not be freely\u00a0given.Ensuring that users do not consent based on time constraints, or the intimidation of the information provided, may require testing with a sample. If the sample is representative, it will give the controller a defense against any claims of\u00a0coercion.The mechanism used for users to signify their consent should be clear. For example, if it is a button, it could read \"I\u00a0consent.\"Controllers can derive clearer potential consequences when the data collected is the same for every consenting user. Users therefore can look over these risks and spend less time making a valid decision. This reduces the chances of users consenting without informing themselves due to the difficult or verbose content\u00a0presented.Based\u00a0on:J. Porekar, A. Jerman-Bla\u017ei\u010d, and T. Klobu\u010dar, \u201cTowards organizational privacy patterns,\u201d Proceedings - The 2nd International Conference on the Digital Society, ICDS 2008,\u00a02008.C. Bier and E. Krempel, \u201cCommon Privacy Patterns in Video Surveillance and Smart Energy,\u201d in ICCCT-2012, 2012, pp.\u00a0610\u2013615.Y. Asnar et al., \u201cInitial Set of Security and Privacy Patterns at Organizational Level,\u201d no. December 2006,\u00a02007.", "tags": "", "title": "Obtaining Explicit Consent"},{"url": "/patterns/Preventing-Mistakes-or-Reducing-Their-Impact", "text": "Numerous services (or products) are designed with the purpose of sharing amongst the public or a specific subset of users. In content sharing implementations, it is commonplace to streamline disclosure so that users do not need to publish manually. Content they generate is often automatically shared with the controller, even if not immediately made available to other users or the public. This of course requires the prior consent of users, though it is also possible for users to forget about that consent, or change their mind. If the distinction lies in a simple setting, it may not be apparent to the user that it is still in\u00a0effect.Immediate and automatic content publication without notification or confirmation of consent leads to unintentional disclosure and may invalidate prior\u00a0consent.Use contextual measures to predict whether content should be processed, re-establishing consent, to prevent accidental\u00a0disclosure.Through the study of patterns in disclosure behavior, systems may be able to helpfully warn users when disclosing following potentially significant change in context, perhaps reducing potential for mistakes. [These] privacy decisions are often correlated with the context of capture and the [content] as indicated [by the user. It] could be feasible to use these patterns for prediction or recommendation of privacy settings. In addition, providing an optional \u201cstaging area\u201d before disclosure actually takes place and an easy way to review recent disclosures may reduce the immediate consequences of quickly regretted or accidental disclosure\u00a0decisions.Clearing up mistakenly shared data adds additional overhead, especially if the service does not offer simple modification or removal of information. As sharing more than actually intended may result in potential damage for users, they will benefit from services which reduce these\u00a0risks.Through the study of [trends] in disclosure behavior, systems may be able to helpfully warn users when disclosing following potentially significant change in context, perhaps reducing potential for mistakes. As [Ahern et al.] found that privacy decisions are often correlated with the context of capture and the content of the photo as indicated by user-specified tags, it could be feasible to use these patterns for prediction or recommendation of privacy settings. In addition, providing an optional \u201cstaging area\u201d before disclosure actually takes place and an easy way to review recent disclosures may reduce the immediate consequences of quickly regretted or accidental disclosure\u00a0decisions.S. Ahern, D. Eckles, N. Good, S. King, M. Naaman, and R. Nair, \u201cOver-Exposed ? Privacy Patterns and Considerations in Online and Mobile Photo Sharing,\u201d CHI \u201907, pp. 357\u2013366,\u00a02007.C. Bier and E. Krempel, \u201cCommon Privacy Patterns in Video Surveillance and Smart Energy,\u201d in ICCCT-2012, 2012, pp.\u00a0610\u2013615.", "tags": "", "title": "Preventing mistakes or reducing their impact"},{"url": "/patterns/Privacy-Policy-Display", "text": "Privacy policies are an important element in the processing activities of a controller. They not only relay to data subjects, the users, crucial aspects about the processing in question, but also adhere to the laws which mandate those policies. Balancing the accessibility of these policies however with the legal comprehensiveness needed is nontrivial. As such users do not naturally familiarize themselves with privacy policies as they need to be verbose, and often complex, to comply with the law. It is therefore necessary that controllers ensure that users are indeed informed before soliciting their\u00a0consent.Whenever the user's information is requested, it must be clear to them exactly what information is needed, who requests it, and what will be done with\u00a0it.As requests for personal data are made, state clearly what information is needed by whom, for which purposes, and by what means, prior to soliciting\u00a0consent.The Article 29 Working Party of the Data Protection Directive of the European Union have set out recommendations regarding the distribution of policy into a layered format. They suggest three tiers, each providing additional detail. Users should have clearly visible access to successive detail upon the controller's request of the related personal\u00a0data.The first tier, 'short notice', shall offer core information necessary for users to understand the purposes and means of processing. It should provide a clear mechanism to obtain further detail. This tier is aimed towards maximum user\u00a0understanding.The second tier, 'condensed notice', includes a summary of pertinent information as required by Article 13 of the General Data Protection Regulation (GDPR), the successor to the Directive. This non-exhaustively includes additional information regarding contact details of applicable entities, legal basis or obligation, legitimate interests, recipients, retention, data subject rights, and whether automated decision making is in\u00a0use.The third tier, 'full notice', presents all remaining information required by the GDPR in addition to the previous information. This is the variation which expresses the full detail of the policy which best holds up the legislative\u00a0requirements.By constantly reminding users what it really means to share their information, they will better contemplate the personal data they choose to provide. However, users may also become fatigued or otherwise desensitized by frequent reminders and begin to overlook privacy policies. As such it is important to balance the levels of visibility and implicit severities of the information\u00a0conveyed.S. Fischer-H\u00fcbner, C. K\u00f6ffel, J.-S. Pettersson, P. Wolkerstorfer, C. Graf, L. E. Holtz, U. K\u00f6nig, H. Hedbom, and B. Kellermann, \u201cHCI Pattern Collection - Version 2,\u201d\u00a02010.C. Graf, P. Wolkerstorfer, A. Geven, and M. Tscheligi, \u201cA Pattern Collection for Privacy Enhancing Technology,\u201d The Second International Conferences of Pervasive Patterns and Applications (Patterns 2010), vol. 2, no. 1, pp. 72\u201377,\u00a02010.", "tags": "", "title": "Privacy Policy Display"},{"url": "/patterns/Awareness-Feed", "text": "An Awareness Feed warns the user about the potential consequences of sharing their personal data. It does so before that data is collected or used, and continues to do so whenever a change in context is detected. This change may include newly provided information by the user, and changes in the environment in which the controller (i.e. provider) operates or processes personal\u00a0data.This pattern allows users to make informed decisions regarding if, when, and how they share their personal data. As more information is collected, the user may become more identifiable, and the data relating to them may become more invasive. Awareness Feed keeps users aware of both the short-term and long-term repercussions in their data sharing\u00a0choices.In a situation where user data is collected or otherwise processed, particularly personal data, many users are concerned about the potential repercussions of their actions. Controllers (e.g.~organizations), which have dynamic and evolving services (or products) which users interact with, may share this concern. This may be for legal, ethical, or public appearance\u00a0reasons.These controllers also care about the monetary implications of a solution, often including the opportunity cost of informed users against the risks and profits of over-sharing. For-profit organizations regularly want to bolster their market share by overcoming competition with state of the art technologies. These changes may have important consequences, unintentional or otherwise, for users of the system. Controllers want to limit the exposure of these risks to their userbase, even if from a third party, as they are responsible for their\u00a0data.Users are often unaware of the privacy risks in their data sharing activities, especially risks which are indirect or long-term. How can we best ensure that users become aware of these\u00a0risks?This problem is agitated by the organizational aim to provide novel and competitive services while keeping users informed. The difficulty of this is frequently underestimated. The pitfalls controllers face as a consequence manifest both in taking shortcuts and in unexpected long-term\u00a0effects.The appeal of convenience features may sway controllers into flawed implementations which undermine user privacy. Automated decisions, influenced by past actions or by other potentially inaccurate metrics, may result in sharing decisions which users do not approve of. The same holds for features which are not adequately assessed. While a controller might intend all the necessary tools for informed decisions to be present, short-sighted process flows may violate user trust all the\u00a0same.Over time, supposedly harmless data may amass into more revealing information, especially when paired with the right metadata. Being able to link user activity to other sources of information may also result in far more exposing situations than\u00a0expected.Not only are users often unaware of the potential consequences of their actions, even controllers themselves regularly fail to anticipate how revealing their services can be. While some users approach this uncertainty with caution, others will risk their privacy in hopes of using the services. Though the uncertainty might not prevent their participation, it may still jeopardize their trust in the\u00a0system.Warn users about potential consequences before collecting or otherwise processing personal data, early enough to be appreciated and late enough to be\u00a0relevant.This information should be provided before the point where privacy risks could materialize. If there is some delay before further processing after collection, the user has some time to review the risks. Until the user accepts them however, that further processing should not take\u00a0place.A concerted effort needs to be made to present the user with unintimidating information relevant to their privacy risks for a service. Providing too much information lessens the chances that the user will read it, while too little information may not properly inform the user. Informing the user too late also puts the user at unnecessary\u00a0risk.By making this effort, the controller avoids accusations of negligence in informing their\u00a0users.Every service which makes use of personal information should be investigated by its creators during its creation, or retrospectively if already available. The controller in question is responsible for this. Not only will this affect the user's understanding once presented to them in layperson terms, but it will also allow the controller to realize the privacy impact of their services. This may encourage them to improve the services to be more respecting of privacy. A good solution composes of accessibility, as well as transparency and\u00a0openness.There needs to be a balance between the user effort required both to use a service and maintain their privacy. Information about the risks should not be deceptive, or difficult for laypersons to comprehend. Meeting this balance may also be challenging, as fully comprehending the risks involved might require a certain understanding of the system\u00a0itself.One way in which to explain the risks involved in a process is through example. This is particularly useful in the case of information aggregation. Visualizing the publicity of data is also useful, users can see how visible information would be, or is, to the outside world. Similar decisions by those who choose to set examples may also help in influencing informed sharing\u00a0behaviour.Users need to be able to trust that a system does not pose unnecessary risks. Fostering a familiarity with openness and transparency about the processes involved may garner this trust. It allows those who invest time an opportunity to be certain, and those who trust in public perception to be at\u00a0ease.The solution of this pattern will cause users to have a better understanding of the potential consequences of information they share. It may empower some to share knowing they may do so safely, though it will cause less activity overall, as many users will be more careful about what they put online. This is not necessarily a negative consequence, though, since regretted decisions merely garner mistrust and prevent future activity. The controller will be able to introduce new services or update old ones with confidence that users are given the opportunity to consider their decisions in full light of the service's potential\u00a0consequences.In addition to lower adoption of risky services, due to public consequences, there will be more cost involved in reworking them. Unattended and system-wide process changes, which negatively affect consequences, will be more difficult to perform. They will not be possible without first disabling the affected services. This is similar to the way some controllers (e.g. Google) handle changes in privacy\u00a0policies.Due to more privacy-minded implementations, the system will not anticipate users as easily, though for many this will be a worthwhile tradeoff. While there is cost in creating good solutions, the long-term cost of bad ones (especially in good faith) can often be\u00a0higher.By informing users prior to the activation of any services which use personal data, many aspects of a system are less fluid and thus require additional forethought. Instead of quick integration into the system, which may have come with many privacy oversights, users will be exposed to consequences that they might not have otherwise realized. Care will need to be taken to ensure that these users do not become overwhelmed. As a consequence of better informed users, however, questionable services are more open to scrutiny and thus many shortcuts will no longer be\u00a0viable.Full adoption of this pattern is not yet commonplace, yet there exist examples of feedback loops to users about activities corresponding to them. This includes notifications such as 'user X wants to access Y', or retrospectively, 'user X accessed Y'. There also exist services which require opt-in, accompanied by explanations of their effects. Conversely telemetry is often opt-out, but occasionally explains what information is at\u00a0stake.G. Aggarwal and E. Bursztein, \u201cAn Analysis of Private Browsing Modes in Modern Browsers.,\u201d USENIX Security \u2026, pp. 1\u20138,\u00a02010.S. Ahern, D. Eckles, N. Good, S. King, M. Naaman, and R. Nair, \u201cOver-Exposed ? Privacy Patterns and Considerations in Online and Mobile Photo Sharing,\u201d CHI \u201907, pp. 357\u2013366,\u00a02007.H. Baraki, K. Geihs, A. Hoffmann, C. Voigtmann, R. Kniewel, B. E. Macek, and J. Zirfas, \u201cTowards Interdisciplinary Design Patterns for Ubiquitous Computing Applications,\u201d Kassel, Germany,\u00a02014.E. S. Chung, J. I. Hong, J. Lin, M. K. Prabaker, J. a. Landay, and A. L. Liu, \u201cDevelopment and Evaluation of Emerging Design Patterns for Ubiquitous Computing,\u201d DIS \u201904 Proceedings of the 5th conference on Designing interactive systems: processes, practices, methods, and techniques, pp. 233\u2013242,\u00a02004.C. Graf, P. Wolkerstorfer, A. Geven, and M. Tscheligi, \u201cA Pattern Collection for Privacy Enhancing Technology,\u201d The Second International Conferences of Pervasive Patterns and Applications (Patterns 2010), vol. 2, no. 1, pp. 72\u201377,\u00a02010.G. Iachello and J. Hong, \u201cEnd-User Privacy in Human-Computer Interaction,\u201d Foundations and Trends\u00ae in Human-Computer Interaction, vol. 1, no. 1, pp. 1\u2013137,\u00a02007.T. Sch\u00fcmmer, \u201cThe Public Privacy \u2013 Patterns for Filtering Personal Information in Collaborative Systems,\u201d in Proceedings of CHI workshop on Human-Computer-Human-Interaction Patterns,\u00a02004.", "tags": "", "title": "Awareness Feed"},{"url": "/patterns/TEST-patterns-stub", "text": "This is a test pattern - used for rendering tests. Please\u00a0ignorelorem\u00a0ipsum", "tags": "", "title": "THIS IS A TEST PATTERN - STUB"},{"url": "/patterns/Asynchronous-notice", "text": "Many sensor related or other recurring forms of data collection are important for improving service (or product) quality, but occur in a manner which is not apparent to the user. Even where the user is informed of such processing, the nature of that processing may cause it to occur within contexts the user would not consent to. Users are also subject to forgetfulness. The controller processing this information therefore seeks to ensure that consent is retained. Some interfaces necessitate more restrictive use of screen real estate, however, and as such can not accommodate extensive information or persistent\u00a0elements.Users being tracked and monitored may not consent to processing they had previously consented to, as the context surrounding that processing is subject to\u00a0change.Also, initial consent may have been forged by an attacker or have been provided by another user of a shared device -- if synchronous notice is only provided at the time of consent, a user may inadvertently distribute personal information over a long period of time after having lost control of their device only\u00a0momentarily.Providing an asynchronous notice requires a reliable mechanism to contact the user (a verified email address or telephone number, for example). Care should be taken to ensure that the mechanism can actually reach the person using the device being tracked. (For example, notifying the owner of the billing credit card may not help the spouse whose location is being surreptitiously\u00a0tracked.)Many repeated notices may annoy users and eventually inure them to the practice altogether. Take measures to avoid unnecessary notices and some level of configuration for frequency of notices. This must be balanced against the concerns of an attacker's opting the user in without their\u00a0knowledge.Whenever there is a context switch, sufficient duration, or random spot check, provide users with a simple reminder that they have consented to specific processing. The triggers and means for contacting the user may be chosen by the user themselves, who should be able to review and if necessary retract their\u00a0consent.Implementation depends on the medium chosen for conveying the notification, and also on the service facilitating collection. For mediums with less space, shorter messages should be provided, but even in more traditionally long-winded options such as email, brevity should be favored. The user should be able to obtain more information by a linking mechanism, also dependent on the medium. The most important information to provide is the fact that they have consented to specific data for specified purposes, and that a context change, spot check, or specified duration has triggered the reminder. Context changes are most notable, as these are most likely to affect the consent. Note that changes to purposes and means instead require new consent, not merely\u00a0notification.Asynchronous notices may also include a summary of the data recently collected (since the last notice, say) in order to provide clarity (and reminders) to the user about the extent of collection. By ensuring that users aren't surprised, asynchronous notice may increase trust in the service and comfort with continued disclosure of\u00a0information.Google Latitude users can configure a reminder email (see below) when their location is being shared with any application, including internal applications like the Location History\u00a0service.Google Location History lets you store your history and see a dashboard of interesting information such as frequently visited places and recent trips.\n   Google Talk Location Status lets you post your location in your chat status.\n   Google Public Location Badge lets you publish your location on your blog or\u00a0site.N. Doty, M. Gupta, and J. Zych, \u201cprivacypatterns.org - Privacy Patterns,\u201d privacypatterns.org, 2017. [Online]. Available: http://privacypatterns.org/. [Accessed:\u00a026-Feb-2015].C. Bier and E. Krempel, \u201cCommon Privacy Patterns in Video Surveillance and Smart Energy,\u201d in ICCCT-2012, 2012, pp.\u00a0610\u2013615.E. S. Chung, J. I. Hong, J. Lin, M. K. Prabaker, J. a. Landay, and A. L. Liu, \u201cDevelopment and Evaluation of Emerging Design Patterns for Ubiquitous Computing,\u201d DIS \u201904 Proceedings of the 5th conference on Designing interactive systems: processes, practices, methods, and techniques, pp. 233\u2013242,\u00a02004.J. Siljee, \u201cPrivacy transparency patterns,\u201d in EuroPLoP \u201915, 2015, pp.\u00a01\u201311.", "tags": "", "title": "Asynchronous notice"},{"url": "/patterns/Added-noise-measurement-obfuscation", "text": "Add some noise to service operation measurements, but make it cancel itself in the\u00a0long-termA service provider gets continuous measurements of a service attribute linked to a service\u00a0individual.The provision of a service may require repeated, detailed measurements of a service attribute linked to a data subject to e.g. properly bill them for the service usage, or adapt the service according to the demand load. However, these measurements may reveal further information (e.g. personal habits, etc.) when repeated over\u00a0time.A noise value is added to the true, measured value before it is transmitted to the service provider, so as to obfuscate it. The noise abides by a previously known distribution, so that the best estimation for the result of adding several measurements can be computed, while an adversary would not be able to infer the real value of any individual measurement. Note that the noise needs not be either additive or Gaussian. In fact, these may not be useful for privacy-oriented obfuscation. Scaling noise and additive Laplacian noise have proved more useful for privacy\u00a0preservation.A service provider can get reliable measurements of service attributes to fulfil its operating requirements; however, no additional personal information can be inferred from the aggregation of several measurements coming from the same\u00a0user.The pattern applies to any scenario where the use of a resource over time is being monitored (e.g. smart grid, cloud computing). The device providing the measurement must be trustworthy, in order to ensure that it abides by the established noise\u00a0pattern.Some information is lost due to the noise added. This loss of information may prevent the information from being exploited for other purposes. This is partly an intended consequence (e.g. avoid discovering user habits), but it may also preclude other legitimate\u00a0uses.In order for information to be useful after noise addition, the number of data points over which measurements are aggregated (i.e. the size of the aggregated user base) needs to be high; otherwise, either the confidence interval would be too broad or differential privacy could not be effectively\u00a0achieved.The pattern applies to any scenario where the use of a resource over time is being monitored (e.g. smart grid, cloud computing). The device providing the measurement must be trustworthy, in order to ensure that it abides by the established noise\u00a0pattern.Some information is lost due to the noise added. This loss of information may prevent the information from being exploited for other purposes. This is partly an intended consequence (e.g. avoid discovering user habits), but it may also preclude other legitimate\u00a0uses.In order for information to be useful after noise addition, the number of data points over which measurements are aggregated (i.e. the size of the aggregated user base) needs to be high; otherwise, either the confidence interval would be too broad or differential privacy could not be effectively\u00a0achieved.An electric utility operates a smart grid network with smart meters that provide measurements of the instantaneous power consumption of each user. The utility employs that information to both adapt the power distribution in a dynamic fashion, according to user demand at each moment, and bill the each client periodically, according to his aggregated consumption over the billing period. However, this information can also be exploited to infer sensitive user information (e.g. at what time he or she leaves and comes back to home,\u00a0etc.)", "tags": "", "title": "Added-noise measurement obfuscation"},{"url": "/patterns/Private-link", "text": "The controller provides a service which hosts resources, potentially constituting personal data. When users want to share (and enable re-sharing of) these resources, they may wish to do so privately using existing communication mechanisms. This is particularly relevant when users are sharing with contacts who would rather not, or cannot, simply\u00a0authenticate.Users want to share a private resource with unauthenticated users in a way that respects the sensitivity of that resource.\nThe solution must not allow users to access resources that weren't intended to be shared, nor publicize the location of the intended resource to unintended\u00a0recipients.Note that the URL will be retained in recipients' browser history and could easily be inadvertently shared with others. Services should help users understand these\u00a0limitations.Services may allow users to revoke existing private links or change the URL to effectively re-set who can access the resource. Additionally, users may set a time-limit for the resource's validity, or have it invalidated upon\u00a0modification.The controller allows their users' online resources to be shared by publishing an unlisted URL with a complex, long, and randomly generated string. This can be part of a query string as opposed to an on disk location. In this case, the preprocessor intercepts the query and redirects the user to the correct resource. This may be an actual file on disk (probably not served by direct link), generated on the fly, or extracted from a database or compressed file. The preprocessor can verify validity dynamically before serving the\u00a0resource.The situation in which the user has a direct link to the resource location is not ideal, however, as it will need to change in the event of a time or version restriction since access to the file is not controlled by the\u00a0preprocessor.Based\u00a0on:", "tags": "", "title": "Private link"},{"url": "/patterns/Data-breach-notification-pattern", "text": "Controllers of services (or products) provided to users collect mass amounts of data, a lot of it personal, to improve the quality and user experience of that service. This is all to be done under the informed consent of the user, who should properly understand the risks involved for their data. One such risk is that of unauthorized access, modification, removal, or sharing of data. If such a data breach occurs, notification is required. Any controller within (or providing services or products within) the EU must notify the supervisory authority of their main establishment or representative. This must occur within 72 hours unless justified. Notifying users is dependent on whether they are sufficiently\u00a0affected.When data breaches occur, numerous risks become apparent for multiple parties, these parties need to be notified and the risks need to be mitigated. Subsequent instances should be prevented through lessons\u00a0learned.Detect and react to data breaches quickly, notifying the supervisory authority of details, particularly risk mitigation, in order to establish whether users must also be informed. Properly handling these events will strengthen user trust rather than weaken\u00a0it.In the event of a breach, the controller should first notify the supervisory authority within 72 hours of it's discovery, and no later without sufficient justification. The processor of personal data, where not also the controller, should notify the controller\u00a0immediately.Notification to the authority should include the nature and extent of the personal data affected, the contact for follow up, likely consequences, and the measures proposed or taken to mitigate the breach's effects. If absolutely necessary these details can be provided as they become available. Any breaches should also be documented for future\u00a0review.Where users are affected in a manner which risks their personal rights and freedoms, they shall also be informed of at least the contact, consequences, and measures to be taken, without undue delay. This is not the case if disproportionate effort would be needed, the data remains protected, or the risk is already sufficiently mitigated. The supervisory authority shall assist in determining whether informing users is\u00a0necessary.Note that associations or other representative bodies may prepare codes of conduct for data breach notifications. These notifications may also be affected by binding corporate rules, or guidelines, recommendations, and best practices from the board, to promote\u00a0consistency.In order to [detect the breach], the [controller] must have in place an access control mechanism and a monitoring mechanism [for personal data]. The pattern cannot ensure that [the relevant] Incident Manager will take adequate actions, hence this process has to be established and controlled by other\u00a0means.Assume a [company] stores all employees' data [through a controller's service]. There is a contractual agreement between [them] that each data leakage is reported within one hour. Now Bob, an employee of [the controller] and not authorized to read [the company's] data, succeeds in circumventing [the] access control mechanisms and reads [personal] data. This represents a data breach of which [the company] has to be notified within an\u00a0hour.\u201cprivacypatterns.eu - collecting patterns for better privacy,\u201d privacypatterns.eu, 2017. [Online]. Available: https://privacypatterns.eu/. [Accessed:\u00a020-Oct-2015].", "tags": "", "title": "Data Breach Notification Pattern"},{"url": "/patterns/Decoupling-[content]-and-location-information-visibility", "text": "Suggested: Retroactive Location\u00a0SharingThe organization in question (likely the controller) does not wish to undermine these expectations, and seeks to enable the user to assign contextually specific privacy\u00a0settings.Concerns about disclosing location information conflict with the appeal of location information for [content]\u00a0organization.Allow users to retroactively decide upon the disclosure of location information with respect to the context of the particular content. Record location information by default, but do not automatically share\u00a0it.Give users an interface or control to configure an access policy regarding the privacy of location information. That is, a place where users may, granularly or in bulk, define who may access location information of their\u00a0content.A basic solution could feature an interface or control for selecting the allowed users from all the types of users of the socially oriented service (e.g. built-in or user-defined groups, individuals, or anonymous users). This control could apply to individual content, or to multiple selections, or\u00a0groups.If a user chooses, certain individuals or groups may have default access to the attached location information. Default access like this, however, invalidates the following\u00a0approach.An extended solution may aim to be further privacy\u00a0preserving.The service may accept ciphertext as the location coupled with the content. When (and only if) the user chooses to make that specific location accessible, their client-side device decrypts the location and provides the service with the plaintext\u00a0location.By applying this pattern the controller prevents location access by default, and thus risks a low location sharing rate. This is due to the tendency of users to leave settings in the default state. However, depending on the effort, the controller may encourage positive public image, and raise adoption\u00a0overall.Based\u00a0on:S. Ahern, D. Eckles, N. Good, S. King, M. Naaman, and R. Nair, \u201cOver-Exposed ? Privacy Patterns and Considerations in Online and Mobile Photo Sharing,\u201d CHI \u201907, pp. 357\u2013366,\u00a02007.", "tags": "", "title": "Decoupling [content] and location information visibility"},{"url": "/patterns/Single-Point-of-Contact", "text": "Personal\u00a0AgentMany controllers make use of a storage platform (i.e. 'cloud' facilities), such as e-Health services that keep their sensitive patient data in a distributed online storage. The sensitivity of this information raises concern and garners a need for special care. The storage medium in this case rules out typical security\u00a0approaches.Effective distributed storage services require specialized privacy management. The deficiencies of traditional means may be expressed through the following:\n- traditional security mechanisms are platform dependent;\n- typically they are difficult to federate or distribute;\n- compliance with protocol can be cumbersome; and\n- as such they are often\u00a0inflexible.Single Point of Contact\u00a0adopts a claim-based approach for both authentication and authorization similar to a super-peer design, also acting as a (Resource) Security Token Service, an Identity and Attribute Provider, and a Relying Party. It features a tried and proven\u00a0expressive e-consent\u00a0language, and can communicate with other SPoCs in a Circle of\u00a0TrustA SPoC is essentially a security authority, which protects patients' privacy in e-Health applications by providing a claim-based authentication and authorisation functionality  (Baier et al. 2010), and facilitating secure communication between an e-Health service and its\u00a0clients.SPoC shares characteristics with a Central Medical Registry (CMReg), which performs authentication and manages identifying access to anonymised medical documents in a central repository. SPoC additionally facilitates secure e-Health service development and integration. It is able to share Electronic Health Records (EHRs) through a peer-to-peer network as an overarching, claim-based, super-peer-like representative of the e-Health community. Multiple SPoCs may also communicate, constituting a Circle of\u00a0Trust.See Fan et al. (2012) Figure 1 for a visual\u00a0depiction.A SPoC is able to issue security tokens as a Security Token Service (STS), authenticate local domain users as an Identity Provider, certify attributes as an Attribute Provider, and accept external claims as a Relying Party. When in a Circle of Trust, the SPoC can also translate the claims of other SPoCs as a Resource\u00a0STS.SPoCs' implementation of e-consent features the following levels, based on Coiera et al. (2004):\n- general consent [with or without specific exclusions];\n- general denial [with or without specific consents];\n- service authorisation;\n- service subscription; and\n-\u00a0investigation.For more information see Fan et al.\u00a0(2012).The SPoC ensures that the privacy of sensitive medical data is protected, and that it is distributed securely and only to the people who are allowed to access the data. However, it requires a reliable credential-based authentication system to be able to validate\u00a0requests.", "tags": "", "title": "Single Point of Contact"},{"url": "/patterns/Privacy-color-coding", "text": "The numerous policies and settings around privacy for each service (or product) used by a user would be quite complex and time consuming if such a user endeavored to investigate them. Policies are written for legal compliance and settings are often configured for best experience rather than privacy. Even in the instances where privacy friendly defaults are used, they may cripple the usability of the system, or otherwise disable desirable features. Some settings can also be difficult to consider due to overly brief and vague\u00a0descriptions.Users do not investigate policies and preferences due to the effort required, and cannot inherently comprehend the consequences of settings otherwise. The poor understanding of these can lead to undesirable\u00a0disclosures.Present the user with standardized color visual cues to help guide them in selecting privacy friendly settings, and in understanding the policies around those\u00a0settings.The results of privacy settings such as visibility are divided into different levels. A distinct color is assigned to each of these levels. Every time the user is performing an action where privacy settings come into play, the color is used as an indication of the privacy settings currently in effect. The choice of colors should take into account prevalent color meanings, like usage of the color red for warning situations. If privacy settings cannot be grouped into distinct levels, a gradient between different colors could also be\u00a0used.The same treatment may be applied to policies, or explanations of settings. User rights and affordances may be presented differently from what the controller may do with their data. Aspects which could be perceived to have the greatest impact on privacy should stand out most. Explanations of who has responsibility or accountability, contact details, etc. can also be given a distinct color. Finally purposes and means for processing should be clearly\u00a0visible.Users receive direct visual cues on the consequences of their privacy settings currently in effect. In order to be more clear about their privacy\u00a0settings.The danger of unwanted actions is decreased, as users will [regularly perceive] visual cues. On the other hand a reduction of complex settings to a few colors may lead to an oversimplification which would render the whole pattern useless. Visual cues must be integrated into the [service] design but must still be placed prominently enough to be noticeable. Cultural aspects for the different meanings of colors should be taken into account. The same color may not be recognized as a warning label in different\u00a0cultures.Alice uses a social network and shares personal stories only with her friends while she shares mundane content publicly. Hence she always has to change the privacy settings of her posts in order to adjust the visibility of the posts. One day she forgets to change the setting and does not realize that she actually shared a precarious story with her\u00a0boss.Christoph Boesch, Frank Kargl, Henning Kopp, and Patrick Mosby, \u201cprivacypatterns.eu - collecting patterns for better privacy,\u201d 2017. [Online]. Available: https://privacypatterns.eu/. [Accessed:\u00a018-Jul-2017].", "tags": "", "title": "Privacy Color Coding"},{"url": "/patterns/Identity-federation-do-not-track-pattern", "text": "All information has been extracted from\u00a0http://blog.beejones.net/the-identity-federation-do-not-track-patternThe Do Not Track Pattern makes sure that neither the Identity Provider\nnor the Identity Broker can learn the relationship between the user\nand the Service Providers the user\u00a0us.This pattern is focused on identity federation\u00a0modelsWhen an identity system provides identifying information about a user\nand passes this to a third party service, different parties can do\ncorrelation and derive additional\u00a0information.Include an orchestrator component, that must act in behalf and be\ncontrolled by the user. The orchestrator makes sure that the identity\nbroker can\u2019t correlate the original request from the service provider\nwith the assertions that are returned from the identity provider. The\ncorrelation can only be done within the orchestrator but that\u2019s no\nissue because this acts on behalf of the user, possibly on the device\nof the\u00a0user.Avoid the correlation of end user and service provider\u00a0dataIn practice, the orchestrator could run in the browser of the user as\na javascript program or as an App on his\u00a0deviceIdentity federations and\u00a0ecosystems", "tags": "", "title": "Identity Federation Do Not Track Pattern"},{"url": "/patterns/Informed-Credential-Selection", "text": "Credential\u00a0SelectionControllers offering services (or products) often provide a means to authenticate users in order to permit them access. This access can be to a secure function, such as fulfilling a transaction. Since this action may have difficult to reverse consequences, the controller needs to be certain of the user's identity and informed consent. In order for consent to be valid, the controller must ensure that it is informed, as well as freely given, specific, and unambiguous. In order to determine identity, however, personally identifying information is needed. Some methods of authentication are also more invasive than others, allowing users to provide more information than\u00a0necessary.Credentials which users supply may be more invasive than necessary, this is a kind of consent which legally must be\u00a0informed.Allow granular credential selection which explains to users the various ways in which personal data can be used, including who may access it, and how it may be used to derive further\u00a0information.One mental model for this could be the use of a credit card for identification. See the HCI Pattern Collection for further information on this\u00a0example.Independent of a mental model, the credential selection UI should contain two steps, namely, selection and summary. During the first step, all graphical elements of the selection mechanism should be based on the mental model. Thus, if working with the card based metaphor this should be apparent from the UI. During the second step, the invoked mental model is not as important as the key issue is to clearly convey which selected data and which meta-data is being\u00a0sent.Allows a user to identify themselves in a granular way, controlling how much information they reveal by doing\u00a0so.This approach should be used to make it easy for users to select the appropriate credentials. It also should inform them about which (personal) data and meta-data the recipient of the information will have after the\u00a0transaction.Jiang et al. (2010). \"A Classified Credential Selection Scheme with Disclosure-minimizing Privacy\". International Journal of Digital Content Technology and its Applications, 4 (9), December 2010. 201 -\u00a0211.S. Fischer-H\u00fcbner, C. K\u00f6ffel, J.-S. Pettersson, P. Wolkerstorfer, C. Graf, L. E. Holtz, U. K\u00f6nig, H. Hedbom, and B. Kellermann, \u201cHCI Pattern Collection - Version 2,\u201d\u00a02010.C. Graf, P. Wolkerstorfer, A. Geven, and M. Tscheligi, \u201cA Pattern Collection for Privacy Enhancing Technology,\u201d The Second International Conferences of Pervasive Patterns and Applications, vol. 2, no. 1, pp. 72\u201377,\u00a02010.", "tags": "", "title": "Informed Credential Selection"},{"url": "/patterns/Appropriate-Privacy-Icons", "text": "Privacy\u00a0IconsControllers offering services (or products) to users have various policies regarding privacy. These typically exist within one document catering to legal evaluation, and thus one which is quite long and complex. Users are often encouraged to read such a policy, though as users are exposed to many of these, they mostly do not. As a countermeasure to this, controllers partition their policies, provide simplified versions, or bring relevant aspects to user attention when needed. One method of simplification is the use of privacy icons. This approach has its own issues for controllers to\u00a0consider.Privacy icons are easily misunderstood, as they are oversimplified concepts using imagery shared with numerous other concepts. Even when fully grasped, important information may be overlooked when finer details play a\u00a0role.Introduce the user to a consistent set of icons, carefully grouped and not excessive, and explain their meaning. Explanations should be short and concise, and these paired with the icons should be put through user tests. Users should be able to understand the icons when shown them in\u00a0context.While these icons should be able to stand alone, it is still important that a user has access to clarification. As such provide a mechanism, such as an on hover tooltip, which further explains what the icon attempts to convey. The icon should also be machine\u00a0readable.When selecting appropriate icons for conveying information, take the following into account:\n- primarily prevent misunderstanding,\n- use icons users are familiar with,\n- do not reassign meaning to familiar icons, and\n- keep icon style and design\u00a0consistent.Perform tests with actual users to determine whether there is any room for misunderstanding and adjust accordingly with further tests. If a concept cannot be reliably conveyed through an icon then it must not be primarily provided as\u00a0one.Regardless of whether an icon perfectly conveys a policy, always allow users to investigate further. This can be achieved through hover, click or tap mechanisms. A tooltip, for example can provide a short explanation, but the full policy being depicted should also be available. As such, a context menu may also be appropriate, especially on single tap for mobile\u00a0users.Currently, most of these are only applied by client-side\u00a0solutions.S. Fischer-H\u00fcbner, C. K\u00f6ffel, J.-S. Pettersson, P. Wolkerstorfer, C. Graf, L. E. Holtz, U. K\u00f6nig, H. Hedbom, and B. Kellermann, \u201cHCI Pattern Collection - Version 2,\u201d\u00a02010.C. Graf, P. Wolkerstorfer, A. Geven, and M. Tscheligi, \u201cA Pattern Collection for Privacy Enhancing Technology,\u201d The Second International Conferences of Pervasive Patterns and Applications, vol. 2, no. 1, pp. 72\u201377,\u00a02010.European Parliament and Council of the European Union, \u201cGeneral Data Protection Regulation,\u201d Official Journal of the European Union,\u00a02015.", "tags": "", "title": "Appropriate Privacy Icons"},{"url": "/patterns/Privacy-Awareness-Panel", "text": "Privacy Awareness Panel in Collaborative\u00a0WorkspaceNumerous services (and products) make an impact on user privacy in ways which are not immediately apparent to the user. Unaware and thus uninformed users are likely to make regrettable decisions in the services they use. Certain kinds of information, especially when combined or viewed over time by others, can reveal details about the user they did not intend. The consent for these disclosures cannot be valid if they do not understand the risks inherent in doing so. Controllers of such personal data therefore seek to minimize these\u00a0risks.Users do not anticipate the pitfalls of disclosure. They may be under the false impression that their activities are inherently\u00a0anonymous.This can manifest in the use of online services where a user shares information with an unknown audience using a pseudonym. Entities within can potentially discover detail the user does not intend, especially if the user loses track of who knows or has access to what. Providing publication history, or reusing aliases in various services, for example can have unintended\u00a0consequences.Furthermore, the controller themselves typically has more capability for identifying the user. If users do not know any better, they might behave or contribute in a manner which assumes they cannot be\u00a0identified.Provide the user with reminders on who can see the content they have or will disclose, what is done with it, why, and how it might become\u00a0identifying.First, it should be made clear to users which persons will be able to access their contributions. Second, users should know that [controllers] get additional information about them for instance their IP addresses, browser versions, location information etc. and thus that they are not completely anonymous [within] the\u00a0[service].The potential consequences of content disclosure may depend on the service in question, and should be investigated in a general\u00a0sense.The user does not need to be shown every potential consequence, but rather must be aware of the need to consider their submission before disclosure. This may require access to an illustrative example to assist in conveying the risks in an accessible\u00a0manner.In a forum setting, a Privacy Awareness Panel may include login and account information, any personalizations, as well as information relating to their browser, session, IP, or other metadata which can uniquely identify them to a degree. It could also show post and user interaction history, and what, if any, of this information is more widely available or public. The panel should be easily located and known about by users, for instance introduced on first use of the forum. Unauthenticated users should also have access to this panel, though there would be less information on these\u00a0users.S. Fischer-H\u00fcbner, C. K\u00f6ffel, J.-S. Pettersson, P. Wolkerstorfer, C. Graf, L. E. Holtz, U. K\u00f6nig, H. Hedbom, and B. Kellermann, \u201cHCI Pattern Collection - Version 2,\u201d\u00a02010.S. P\u00f6tzsch, P. Wolkerstorfer, and C. Graf, \u201cPrivacy-awareness information for web forums,\u201d Proceedings of the 6th Nordic Conference on Human-Computer Interaction Extending Boundaries - NordiCHI \u201910, no. June, pp. 363\u2013372,\u00a02010.C. Graf, P. Wolkerstorfer, A. Geven, and M. Tscheligi, \u201cA Pattern Collection for Privacy Enhancing Technology,\u201d The Second International Conferences of Pervasive Patterns and Applications (Patterns 2010), vol. 2, no. 1, pp. 72\u201377,\u00a02010.G. Aggarwal and E. Bursztein, \u201cAn Analysis of Private Browsing Modes in Modern Browsers.,\u201d USENIX Security \u2026, pp. 1\u20138,\u00a02010.", "tags": "", "title": "Privacy Awareness Panel"},{"url": "/patterns/Informed-Secure-Passwords", "text": "Secure\u00a0PasswordsCredentials are required by numerous services (and products) in order to ensure that only authenticated and authorized users have access to certain features. Controllers typically provide authentication mechanisms in the form of usernames and passwords. Although these provide a weak form of security when used incorrectly, they are more convenient for users than many less popular and more secure alternatives. Controllers often try to circumvent the shortcomings of passwords by encouraging users to change them frequently, use stronger variations, check them, and prevent disclosure and reuse. However users make use of many services, and use many passwords, thus discouraging proper application. This misapplication can result in personal data being accessed by unauthorized\u00a0persons.Users must regularly maintain many strong passwords, remember them, and protect them, but are not well equipped to do so. So instead many choose weak ones and reuse\u00a0them.Provide users with assistance in understanding and maintaining strong passwords which are easier to\u00a0remember.Short passwords, those at character lengths which are feasible for brute forcing, are not secure. The difficulty to brute force is affected by the known complexity, such as using a variety of character types. However, complexity affects password memorability more than strength. It is more important that passwords are long enough, and varied enough. This does not mean they should be difficult to remember. A couple unrelated words strung together can be a very secure yet memorable\u00a0password.These aspects can be weighted together to provide the user with a strength meter, as well as the explanations behind it. Examples of secure passwords should also be provided, but not accepted as the actual password. Do not enforce the use of special characters and numbers. Length, along with sufficient variation, should be the deciding factor in password\u00a0strength.Given enough resources and time however, state of the art character lengths can be overcome. It is as such useful to change them more regularly than the time it would take to brute force them. Otherwise, the longer that a password remains unchanged, the more likely it is that the password has been\u00a0compromised.When verifying whether a user used the same password in a second field, to prevent mistypes, simply indicate whether the fields match with a recognizable affirmation. Typically this uses a green theme, and may use a check\u00a0mark.Secure passwords are very important in [an interconnected world]. Users generally tend to use familiar words such as names of pets and family members and no special [characters] when creating a password. These passwords can hence be easier hacked using social engineering than longer [and more complex passwords]. Secure passwords are a necessary step towards personal security. Using the above approach, the user obtains more feedback on the safety of the entered password and is therefore able to create safe passwords that can be\u00a0remembered.Strongpasswordgenerator.com both provides explanation on state of the art approaches to secure passwords in a layperson friendly manner and helps generate\u00a0them.S. Fischer-H\u00fcbner, C. K\u00f6ffel, J.-S. Pettersson, P. Wolkerstorfer, C. Graf, L. E. Holtz, U. K\u00f6nig, H. Hedbom, and B. Kellermann, \u201cHCI Pattern Collection - Version 2,\u201d\u00a02010.C. Graf, P. Wolkerstorfer, A. Geven, and M. Tscheligi, \u201cA Pattern Collection for Privacy Enhancing Technology,\u201d The Second International Conferences of Pervasive Patterns and Applications, vol. 2, no. 1, pp. 72\u201377,\u00a02010.", "tags": "", "title": "Informed Secure Passwords"},{"url": "/patterns/Lawful-Consent", "text": "This pattern covers in detail the legal and social obligations surrounding a data subject's consent to processing of their data in specific circumstances. Every use of the subject's personal data should be covered by an explicit agreement in which the data subject was made aware of the implications of their\u00a0consent.Where data controllers (e.g. organisations) aim to provide a service (or product) to users, there may be opportunities to reuse data, gather feedback, or make use of user data to further their system's value. Many controllers seek to continually collect and utilise this data, often in ways which warrant privacy concerns. For any data processing (including collection), controllers should first obtain consent from the users in\u00a0question.There are social norms surrounding the use of personal data which need to be adhered to if an controller wishes to avoid scrutiny. Users do not inherently trust controllers to handle their personal data with care for privacy. Without clearly defined boundaries, these users may have justifiable concerns about what is learned about them, and how this information may be used. Additionally, various jurisdictions supply varying compliance requirements, and these controllers need to cater to every market they provide\u00a0to.Doing otherwise, possibly by disinterest or negligence, may have financial consequences in addition to potential public outcry. Despite this, controllers regularly consider the impact that their decisions may have on competitive edge and resulting profits. The link between better decision making, possibly less sharing, and reduced monetary gains sways some controllers into unlawful forms of\u00a0consent.An controller aims to maximise the value of their services by gathering as much sharing and participation as possible, potentially seeing user consent as a barrier to functionality and efficiency. They may inadvertently subvert notions of consent by unnecessarily bundling together desirable services with needs for personal information, or downplaying the significance of the data involved. They undermine self-determination at the risk of losing trust from their users, and attracting legal investigations which may rule their practices\u00a0unlawful.A user should be given every opportunity to assess their sharing choices prior to making their consent. The controller should aid the user in comprehending the tradeoffs apparent in using each of their services, without over-burdening the user. These consented services should be purposed-separated, so that users may make use of functionality without first granting unnecessary\u00a0consent.Services should be separated into distinct processes for which distinct consent is acquired. Each purpose requires its own consent. These permissions need to be given subsequent to ascertaining sufficient awareness in the user about the consequences of that\u00a0consent.The users should not be pressured into providing consent. Instead, the benefits may be presented along with the trade-offs so that the users may make an informed decision. Some users are not necessarily capable of making these decisions themselves (e.g. children) and thus provisions need to be made to cater to this. The provided information should not be misleading, as coerced consent is not a valid form of permission. One way to present policies in an accessible manner is through comparative examples (e.g. in addition to further detail, what is unique about our privacy\u00a0policy?).In more personal services (i.e. one-on-one), personal privacy policies may undergo a formal negotiation. As opposed to user preferences (both at sign-up and through appropriate defaults), understanding a user's personal privacy requirements may benefit from the facilitation of a human representative. This, however, suffers from it's own drawbacks where the representative may misunderstand the user's requirements. Even in interpersonal exchanges, controllers should err on the side of caution. Where available, explicit signing of an agreement aids in proving\u00a0consent.With the ability to choose exactly what tradeoffs are agreeable to them, users will be more content, and trusting of the system. They may as a result use more services, and participate more than they otherwise would. Being aware of what information is actually needed to perform certain functionality may also prevent its use, but rightfully so as to prevent\u00a0backlash.The need for certain information for some services will bring inappropriate business processes to the foreground to be rectified, or otherwise questioned. This will likely bring the controller towards better practices, and may affect others as well. Once the public sees the controller's willingness to cooperate, trust will grow even\u00a0further.Overall adoption will grow for controllers who are shown to be trustworthy and upfront about their data processing practices. This may very well offset the costs involved in maintaining\u00a0transparency.Allowing informed and specific consent prevents controllers from soliciting misplaced consent, which greatly reduces the adoption of invasive services. These are often the most profitable\u00a0services.Explicit Consent / Obtaining Explicit consent via privacy agreement / Permission to Use Sensitive Data via Privacy Agreement / Sign an Agreement to Solve Lack of Trust on the Use of Private Data\u00a0ContextArticle 29 Data Protection Working Party. (2007). Opinion 4/2007 on the concept of personal data. Working Party Opinions. Retrieved from\u00a0http://scholar.google.com/scholar?hl=en&btnG=Search&q=intitle:Opinion+4+/+2007+on+the+concept+of+personal+data#0J. Porekar, A. Jerman-Bla\u017ei\u010d, and T. Klobu\u010dar, \u201cTowards organizational privacy patterns,\u201d Proceedings - The 2nd International Conference on the Digital Society, ICDS 2008,\u00a02008.C. Bier and E. Krempel, \u201cCommon Privacy Patterns in Video Surveillance and Smart Energy,\u201d in ICCCT-2012, 2012, pp.\u00a0610\u2013615.", "tags": "", "title": "Lawful Consent"},{"url": "/patterns/Minimal-Information-Asymmetry", "text": "Users frequently interact with controllers whose services (or products) they have not used before. At this point the knowledge the user has about the controller and its practices, especially regarding privacy, is typically nonexistent. The controller as a whole has a much clearer understanding of its policies. It also begins to know a lot about the user in a short time period, if not already well informed. The user needs to put in sufficient effort to investigate the controller to know about its practices to provide valid consent. The controller needs this valid consent to lawfully process the user's\u00a0information.Controllers have far more information than the users who utilize their services, which makes the users vulnerable to\u00a0exploitation.Require minimal information from the user, so that only as much personal data as is required, explained, and consented to, is processed. Further reduce the imbalance of policy knowledge by writing clear and concise policies rather than, or in addition to, complex and verbose\u00a0ones.Limit the amount of data needed to provide the services necessary to the users, and where appropriate, prefer less sensitive data to do so. Give users the option to opt in to features which require more data, but keep it minimal by default. If the amount of data needed is minimized, then users have less they need to understand, and less to disagree with. This also allows for more simple\u00a0policies.Making policies more clear and concise is also crucial, as users will not want to sift through long-winded texts to understand what would happen with their data. Highlight important aspects for users themselves, rather than allowing them to become cluttered with legal jargon, detail, and complexity. While certain elements cannot be explained adequately without doing so at length, not all aspects are relevant at once. Some elements may be summarized without the detail, so that users may better understand the current focus. The full detail should still exist however, and be easily\u00a0located.Many online organizations provide signals to their customers. Often they are publicly and freely available, but can also be purchased by third parties. The online auction site, eBay, for example, uses a reputation system to assist other buyers in feeling more comfortable purchasing from an unknown seller. Many other ecommerce sites (such as Amazon) rely heavily on the reputation and referral systems in order to help customers make a more informed\u00a0decision.S. Romanosky, A. Acquisti, J. Hong, L. F. Cranor, and B. Friedman, \u201cPrivacy patterns for online interactions,\u201d Proceedings of the 2006 conference on Pattern languages of programs - PLoP \u201906, p. 1,\u00a02006.S. Engelman, L. F. Cranor, A. Chowdury, \u201cAn analysis of P3P-Enabled web sites among Top-20 Search Results\u201d Carnegie Mellon University,\u00a02005.L. Cranor, J. Reagle, M. Ackerman, \u201cBeyond Concern: Understanding Net Users\u2019 Attitudes About Online Privacy\u201d AT&T Labs,\u00a01999.", "tags": "", "title": "Minimal Information Asymmetry"},{"url": "/patterns/Location-granularity", "text": "Support minimization of data collection and distribution. Important when a service is collecting location data from or about a user, or transmitting location data about a user to a\u00a0third-party.When a service is collecting location data from or about a user, or transmitting location data about a user to a\u00a0third-party.Many location-based services collect current or ongoing location information from a user in order to provide some contextual service (nearest coffee shop; local weather; etc.). Collecting more information than is necessary can harm the user's privacy and increase the risk for the service (in the case of a security breach, for example), but location data may still need to be collected to provide the service. Similarly, users may want the advantages of sharing their location from your service to friends or to some other service, but sharing very precise information provides a much greater risk to users (of re-identification, stalking, physical intrusion,\u00a0etc.).Accepting or transmitting location data at different levels of granularity generally requires a location hierarchy or geographic ontology agreed upon by both services and a more complex data storage model than simple digital\u00a0coordinates.  Truncating latitude and longitude coordinates to a certain number of decimal places may decrease precision, but is generally not considered a good fuzzing algorithm. (For example, if a user is moving in a straight line and regularly updating their location, truncated location information will occasionally reveal precise location when the user crosses a lat/lon boundary.) Similarly, using \"town\" rather than lat/lon may occasionally reveal more precise data than expected when the user crosses a border between two\u00a0towns.Fire Eagle specifically requires that recipient applications be written to handle data at any of the levels, and allows updating the user's location at any level of\u00a0granularity.One of the fore-runners to the W3C Geolocation API, Firefox's experimental Geode feature allowed JavaScript access to the current location at four different levels of\u00a0granularity.", "tags": "", "title": "Location Granularity"},{"url": "/patterns/Protection-against-tracking", "text": "This pattern avoids the tracking of visitors of websites via cookies.\nIt does this by deleting them at regular intervals or by disabling\ncookies\u00a0completely.This pattern is applicable when personal identifiable information is\ntracked through software tools, protocols or mechanisms such as\ncookies and the\u00a0like.With every single interaction in the web you leave footmarks and clues\nabout yourself. Cookies for example enable webservers to gather\ninformation about web users which therefore affects their privacy and\nanonymity. Web service providers trace user behavior, which can lead\nto user profiling. Also providers can sell the gathered data about\nusers visiting their pages to other\u00a0companies.Restricting usage of cookies on the client side by deleting cookies on\na regular basis e.g. at every start-up of the operating system or\nenabling them case-by-case by deciding if the visited website is\ntrustworthy or not and by accepting a cookie only for the current\nsession. At the highest level of privacy protection cookies are\ndisabled, but as a consequence web services are restricted. Another\nsolution could be that cookies are exchanged between clients, so that\nsophisticated user profiles\u00a0emerge.Restricting a website to not be able to track any of the user's\npersonal identifiable\u00a0informations.With cookies disabled there is no access to sites that require enabled\ncookies for logging in. Other tracking mechanisms for user\nfingerprinting may still work even when cookies are\u00a0disabled.Alice wants to buy shoes and she wants to shop online. She heads to an\nonline shop and searches for shoes but can\u2019t decide which ones she\nwants, so she buys neither of them. The next day she finds a couple of\nemails in her inbox, giving her suggestions for other shoes and\nalerting her that the viewed shoes are now on\u00a0sale.Junkbuster is an old proxy filtering between web server and browser to\nblock ads and cookies, but it is no longer maintained. A program named\nCookieCooker (http://www.cookiecooker.de/) provides protection for\nmonitored user behaviour and interests by exchanging cookies with\nother users or using a random selection of identities. Unfortunately\nthis project also seems to be not maintained anymore. There is also\nthe Firefox Add-on Self-Destructing Cookies which deletes cookies of\ntabs as soon as they are\u00a0closed.", "tags": "", "title": "Protection against Tracking"},{"url": "/patterns/Unusual-activities", "text": "Handling unusual account activities with multiple\u00a0factorsServices (or products), particularly over the Internet, tend to use username and password based authentication. This security mechanism proves most convenient for users, as it is commonplace and simple compared to the more secure alternatives. It is also subject to common shortcomings, however. Passwords become less secure the longer they remain unchanged, are often vulnerable to brute force, snooping, and phishing attacks, and cannot be proven to be held solely by the\u00a0user.This complicates the certainty of the authentication, and thereby the authenticity of any decision made by the user, including consent. Controllers may also derive additional factors, however, such as device or access specific information. If location is provided, for example, it may hint at unlikely account\u00a0activity.Username and password authentication alone has varying reliability for proving decisions taken by a user, especially when concerning more sensitive actions. Controllers need to enhance their certainty that any consent provided is\u00a0legitimate.A balance should be made between the insecurity of username and password authentication and the inconvenience of multi-factor authentication. If measures affect usability or privacy too greatly, users will stop using the system. While the rate of false positives must not be too high, they are far preferable to undetected\u00a0intrusions.Analyze the available information for which there is consent to establish an access norm. Test this against future access to identify unusual activities. When this occurs, alert the user and use multi-factor authentication while re-establishing certainty. The authenticated user should be able to review and take further\u00a0action.By running native code, the application can [consensually] collect some [device identifiers], including the operating system environment settings (e.g. the list of running processes), the hardware parameters (such as the ID of the CPU), and device UUIDs (provided by mobile operating systems like iOS). By completing a network request, the service also retrieves the IP address of the [device]. [These] can be used to tell if a [device] is 'new' to the service. The service can have its rules to determine if a sign-in is 'suspicious', for example, an access from a new country / [device] / operating system is considered\u00a0suspicious.In case of a suspicious [activity], multi-factor authentication may be a way to let the legitimate user in. The service can request [further authentication], such\u00a0as:Using multi-factor authentication only in case of suspicious [activity] is more convenient [than] using it all the time, but is less\u00a0secure.When a suspicious sign-in is detected, it may be a sign that the password has already been leaked. Depending on the type of the service, it can notify the user about the suspicious sign-in through E-mail, telephone, or other\u00a0means.Here the immediate notification can also be used in the multi-factor\u00a0authentication.For services that can be logged on from multiple devices at the same time, the user should be able to check the existence of other sessions, and review recent\u00a0[activity].Users will be able to use an easier, more familiar method of authentication in most scenarios, only having to resort to multi-factor authentication when there is potential cause for\u00a0concern.This pattern has some limitations. For example, it relies on accurate identification of suspicious [activity] based on meta information, where the meta information including the IP address can be spoofed by an experienced\u00a0attacker.If the fallback multi-factor authentication only happens occasionally to the legitimate account owner, they may be unprepared to [handle] such authentication, leading to [decreased]\u00a0usability.Gmail displays information about other sessions (if any) in the footer, linking to a page named \"Activity on this account\" which lists other sessions and recent activities to the Gmail account. The user has the option to sign out other\u00a0sessions.In case of annoying false positives, the user may choose to disable the alert for unusual activity. The disable takes about a week, \"to make sure the bad guys aren't the ones who turned off your\u00a0alerts.\"When Facebook detects an unusual sign-in, it shows 'social authentication' that displays a few pictures of the user's friends and asks the user to name the person in those\u00a0photos.The 'Security' tab of the 'Settings' of the Dropbox website displays all web browser sessions logged in to the account, and enables the user to log out one or more of them. The name of the browser, operating system, and the IP address and corresponding country are displayed to help the user make a\u00a0choice.It also displays all devices that are linked to the account, and allows the user to unlink one or more of\u00a0them.N. Doty, M. Gupta, and J. Zych, \u201cprivacypatterns.org - Privacy Patterns,\u201d privacypatterns.org, 2017. [Online]. Available: http://privacypatterns.org/. [Accessed:\u00a026-Feb-2015].J. Polakis, M. Lancini, G. Kontaxis, F. Maggi, S. Ioannidis, A. Keromytis, and S. Zanero, \u201cAll Your Face Are Belong to Us: Breaking Facebook\u2019s Social Authentication,\u201d Proceedings of the Annual Computer Security Applications Conference (ACSAC), no. i, pp. 399\u2013408,\u00a02012.[Gmail] displays unusual activities regarding an account, which involves identifying unusual activities where the password entered is correct. For some other services, correct passwords can be rejected from a new device /\u00a0location.So, the scope of this pattern is to handle unusual activities (including\u00a0sign-ins).This pattern includes multi-factor authentication and two-step authentication, which are well studied. But the general topic about informing the user of unusual activities seems to be [lacking in]\u00a0literature.", "tags": "", "title": "Unusual Activities"},{"url": "/patterns/Appropriate-Privacy-Feedback", "text": "Notification on Access of Personal\u00a0DataUsers are frequently unaware or unsure about what personal data systems collect and otherwise process. When systems fade into the background users are less likely to take notice and adjust what information is collected. Data controllers who provide services (or products) to such users realize that consent is not valid without users first being sufficiently informed. They aim to do so in a manner which is appropriate for the\u00a0service.The controller may have relied on op-out mechanics, but now realizes that within the European General Data Protection Regulation (recital 32) 'silence, pre-ticked boxes or inactivity' no longer constitute consent. Unnecessarily disruptive notice is also not\u00a0permitted.Many systems are designed to be seamless or ubiquitous. However, this can make personal data risks less apparent to the\u00a0user.As a result users may overlook services without fulling understanding the privacy risks involved. Potentially, these users may realize consequences long after, or worse, not realize them at\u00a0all.Visible feedback loops, which capture the user's attention, are needed to help ensure that users understand what data is being collected, who can see that data, and how might it be\u00a0used.Notification should occur before access where possible, and during or shortly after access if earlier notification is not appropriate. In most cases this means preventing a user's use of a service before allowing the core functionality of the service to run at all. Where some features with variable privacy implications are not essential to the service, they may be provided as optional, defaulting to being\u00a0disabled.Where users choose to be notified less immediately or less often, and after being warned of the risks involved, then the service may store logs of its privacy affecting activities. The user should then be able to retrieve these logs, in a human readable form, at will. As only the user should be able to access these, unless said user provides informed consent otherwise, it should also be secured. Use state of the art means of encryption to do this. If this functionality cannot be done in this manner, due to technical constraints for example, then do not provide logging\u00a0functionality.The user will be informed before using a service, which will cause the user to be more careful according to their personal privacy preference. Those who find the service too invasive will not use it, or provide feedback towards its improvement. The service will not be liable for user activities where it has informed them of the risks those activities\u00a0involve.Preventing functionality until consent is acquired lessens the feasibility of various services. However, doing otherwise presents risks of high financial and good-will\u00a0damages.When you share some content on Facebook, it sometimes asks you to review your fundamental privacy settings. In the short tour given, you can see what data is accessed by other users or by third party\u00a0applications.E. S. Chung, J. I. Hong, J. Lin, M. K. Prabaker, J. a. Landay, and A. L. Liu, \u201cDevelopment and Evaluation of Emerging Design Patterns for Ubiquitous Computing,\u201d DIS \u201904 Proceedings of the 5th conference on Designing interactive systems: processes, practices, methods, and techniques, pp. 233\u2013242,\u00a02004.H. Baraki et al., Towards Interdisciplinary Design Patterns for Ubiquitous Computing Applications. Kassel, Germany,\u00a02014.G. Iachello and J. Hong, \u201cEnd-User Privacy in Human-Computer Interaction,\u201d Foundations and Trends\u00ae in Human-Computer Interaction, vol. 1, no. 1, pp. 1\u2013137,\u00a02007.", "tags": "", "title": "Appropriate Privacy Feedback"},{"url": "/patterns/Sticky-policy", "text": "Machine-readable policies are sticked to data to define allowed usage\nand obligations as it travels across multiple parties, enabling users\nto improve control over their personal\u00a0information.Multiple parties are aware of and act according to a certain policy\nwhen privacy-sensitive data is passed along the multiple successive\nparties storing, processing and sharing that\u00a0data.Data may be accessed or handled by multiple parties that share data\nwith an organisation in ways that may not be approved by the data\u00a0subject.Service providers use an obligation management system. Obligation\nmanagement handles information lifecycle management based on\nindividual preferences and organisational policies. The obligation\nmanagement system manipulates data over time, ensuring data\nminimization, deletion and notifications to data\u00a0subjects.The goal of the pattern is to enable users to allow users to control\naccess to their personal\u00a0information.Bene\ufb01ts: Policies can be propagated throughout the cloud to trusted\norganisations, strong enforcement of the policies, traceability.\nLiabilities: Scalability: policies increase size of data. Practicality\nmay not be compatible with existing systems. It may be difficult to\nupdate the policy after sharing of the data and existence of multiple\ncopies of data. It requires ensuring data is handled according to\npolicy e.g. using\u00a0auditing.When data is shared by an organisation they can use privacy preserving\npolicy to enforce respecting user privacy by third party organisations\nthat use, process and store such data. For example, a hospital may\nshare data with third party organisations requiring adhering to\nspecific privacy policies associated with the\u00a0data.", "tags": "", "title": "Sticky Policies"},{"url": "/patterns/Outsourcing-[with-consent]", "text": "Controllers often do not have the means to feasibly or sufficiently process the data they oversee to the extent they desire. In these cases they seek an external processor or third party to handle the process. This typically conflicts with their already obtained consent from their users (their data subjects), as further processing by a third party is not necessarily compatible with the agreed upon purposes. In these situations the controller does not have legally obtained consent for this processing and will be liable if they carry it\u00a0out.Third party processors do not inherent user consent granted to a controller, but need each user's consent before they may process their information. The processor cannot contact the necessary users as they have no lawful access to any means to identify\u00a0them.Obtain additional (Lawful Consent)[Lawful-Consent] for the specific purposes needed from each user before allowing the third party to process their data. Do not process the data of users who do not\u00a0consent.The consent can be seen as a contract establishing what and how data will be processed by the [third party]. The [controller] must also ensure, preferably by a written agreement, that the [third party] strictly follows all conditions relating to data processing that were imposed on\u00a0[them].Figure 2(b) shows an SI* model explaining the solution of Compagna et al.\u00a0(2007)The pattern solves the problem of granting [the consent] necessary to perform out-sourced data processing by assuring [users that their information is] processed according to the\u00a0contract.The scenario described by Compagna et al. (2007) features a Health Care Centre (data controller) and a user (data subject), Bob, who needs constant supervision. The subcontractor, a Sensor Network Provider (third party supplier), installs and maintains the network responsible for automated monitoring of Bob's health. This subcontractor needs additional specific, informed, explicit, and freely given consent from\u00a0Bob.L. Compagna, P. El Khoury, F. Massacci, R. Thomas, and N. Zannone, \u201cHow to capture, model, and verify the knowledge of legal, security, and privacy experts : a pattern-based approach,\u201d ICAIL \u201907,\u00a02007.", "tags": "", "title": "Outsourcing [with consent]"},{"url": "/patterns/Anonymity-set", "text": "This pattern aggregates multiple entities into a set, such that they\ncannot be distinguished\u00a0anymore.This pattern is applicable in a messaging scenario, where an attacker\ncan track routing information. Another possible scenario would be the\nstorage of personal information in a\u00a0database.In a system with different users we have the problem that we can often\ndistinguish between them. This enables location tracking, analyzing\nthe behaviour of the users or other privacy-infringing\u00a0practices.There are multiple ways to apply this pattern. One possibility is, to\nstrip away any distinguishing features from the entities. If we do not\nhave enough entities, such that the anonymity set would be too small,\nthen we could even insert fake\u00a0identities.The goal of this pattern is to aggregate different entities into a\nset, such that distinguishing between them becomes\u00a0infeasible.One factor to keep in mind is that this pattern is useless if there\nare not many entities, such that the set of probable suspects is too\nsmall. What \"too small\" means depends on the exact scenario. Another\nfactor is a possible loss of\u00a0functionality.Assuming that there are two companies, one is a treatment clinic for\ncancer and the other one a laboratory for research. The Clinic\nreleases its Protected Health Information (PHI) about cancer victims\nto the laboratory. The PHI's consists of the patients' name, birth\ndate, sex, zip code and diagnostics record. The clinic releases the\ndatasets without the name of the patients, to protect their privacy. A\nmalicious worker at the laboratory for research wants to make use of\nthis information and recovers the names of the patients. The worker\ngoes to the city council of a certain area to get a voter list from\nthem. The two lists are matched for age, sex and location. The worker\nfinds the name and address information from the voter registration\ndata and the health information from the patient health\u00a0data.Anonymity sets are in use in various routing obfuscation mechanisms\nlike Onion Routing. Hordes is a multicast-based protocol that makes\nuse of multicast routing like point-to-multipoint delivery, so that\nanonymity is provided. Mix Zone is a location-aware application that\nanonymizes user identity by limiting the positions where users can be\u00a0located.", "tags": "", "title": "Anonymity Set"},{"url": "/patterns/Trustworthy-privacy-plugin", "text": "Aggregate usage records at the user side in a trustworthy\u00a0manner.A service provider gets continuous measurements of a service attribute linked to a service individual. Applicable service tariffs may vary over\u00a0time.The provision of a service may require repeated, detailed measurements of a service attribute linked to a data subject to e.g. properly bill them for the service usage. However, these measurements may reveal further information (e.g. personal habits, etc.) when repeated over\u00a0time.Host a Privacy Plugin at a consumer-trusted device, in between the metering and the billing systems. and the service provider in charge of billing for the service usage. This privacy plugin, under the consumer\u2019s control, computes the aggregated invoice and sends it to the service provider (or to its billing subsystem), which does not need any fine-grained consumption records anymore. Cryptographic techniques (homomorphic commitments, zero-knowledge proofs of knowledge, digital signatures) are used to ensure trustworthiness of the generated invoices without requiring tamper-proof\u00a0hardware.A service provider can get a trustworthy measurement of service usage along a period to issue a bill for the service usage; however, the detailed consumption for finer intervals cannot be\u00a0obtained.The service provider does not need anymore to access detailed consumption data in order to issue reliable\u00a0bills.An electric utility operates a smart grid network with smart meters that provide measurements of the instantaneous power consumption of each user. Depending on the power demand, dynamic tariffs are applied. The utility employs that information to bill each client periodically, according to his aggregated consumption over the billing period and the respective tariffs at each moment. However, this information can also be exploited to infer sensitive user information (e.g. at what time he or she leaves and comes back to home,\u00a0etc.)", "tags": "", "title": "Trustworthy Privacy Plug-in"},{"url": "/patterns/User-data-confinement-pattern", "text": "Avoid the central collection of personal data by shifting some amount\nof the processing of personal data to the user-trusted environments\n(e.g. their own devices). Allow users to control the exact data that\nshares with service\u00a0providersThis pattern may be used whenever the collection of personal data with\none specific and legitimate purpose still pose a relevant level of\nthreat to the users'\u00a0privacyThe engineering process is biased to develop system-centric\narchitectures where the data is collected and processed in single\ncentral entities, forcing users to trust them and share potentially\nsensible personal\u00a0dataThe solution is to shift the trust relationship, meaning that instead\nof having the customer trust the service provide to protect its\npersonal data, the service provider now haves to trust the customers'\u00a0processing.In the smart meter example, the smart meter would receive the monthly\ntariff and calculate the customer's bill which will be then sent to\nthe energy provider where it will be processed. The main benefit is\nthat at no moment the personal data has left the users trusted\u00a0environment.Avoid the need for trust in service providers and the collection of\npersonal\u00a0dataDepending on the type of processing (e.g calculate the bill for the\nmonthly energy consumption or the age from the birth date) the service\nprovider will require some guarantees from the processor (the end\nuser). This may involve the usage of Trusted Platform Modules or\ncryptographic algorithms (e.g.\u00a0ABC4Trust)The smart grid is a domain with a clear example: having smart meters\ndelivering hourly customers' energy consumption to the energy provider\nposes a serious threat to the customers' privacy. If the only purpose\nof collecting these data is to bill the customer, why cannot this\ncalculation be done by the customer based on pre-established\u00a0tariffs?Similar examples in other domains are \"pay as your drive\" insurance\npolicies where the insurance price is calculated based on the drivers\nbehaviour or electronic toll\u00a0pricingSmart meter, Privacy-enhanced attribute based credentials, pay as your\ndrive insurances, electronic toll\u00a0pricing", "tags": "", "title": "User data confinement pattern"},{"url": "/patterns/Privacy-Mirrors", "text": "Controllers process a lot of personal data within the services (or products) which users use. These users should however be made to understand the risks involved in all the processing. Typically users need to be encouraged to review what data a service uses, and whether they consent to this. When provided with lock icons for certificates, or privacy coordination through color, users often still overlook warnings. Users want information to be streamlined, quick, and easy to digest in order to benefit from a service without\u00a0delay.This pattern is focused on the socio-technical domain, as opposed to purely technical, and as such considers a number of factors that do not play into a developer's\u00a0perspective.Users are frequently unaware of the personal data which a system processes and may use to draw conclusions from. Due to this, they either accept their data's undefined usage, or limit their disclosure, potentially more than needed, which could result in an poorer user\u00a0experience.An approach to this transparency which is both noticeable and yet unobtrusive is needed. One which is passively assertive. A purely technical solution is not appropriate, it must also consider physical and social\u00a0contexts.This is because personal information has varying levels of sensitivity to users depending upon these contexts (for e.g. a closed room, or a close friend, are contexts in which some personal information may be considered less\u00a0vulnerable).This pattern encourages methods, mechanisms, and interfaces which reflect the history, flow, state, and nature of processed personal data which may otherwise have been\u00a0hidden.Logging is possible in technical systems from as little to as much granularity as desired. Whatever is kept needs to have been done so bearing in mind the social implications, i.e. the contexts, which may be important to a user's privacy. Who was involved in the processing, where was it processed, when, how, why, etc. are\u00a0relevant.The past must be summarized in a way which is easy to understand, but still detailed enough to identify less obvious risks. What information is relevant to different users as opposed to that which is seen as unnecessary noise? What isn't socially acceptable to record? How long must this be kept, should it deteriorate or simply\u00a0vanish?Logging is of little (or detrimental) use if not transparent and appropriately accessible. There needs to be a way to disseminate this history, state, and flow information to the users without inducing notification fatigue and without exposing information which is not contextually acceptable. Visual cues may be less distracting than the use of other senses, and capable of conveying much more information. However, some contexts may call for more distracting notification. A user should be able to choose whether, how much, and by what means they are notified - as some people have higher tolerances, or different tolerances, for distraction than\u00a0others.A distinction is suggested between notifications which require 'glancing', 'looking', or 'interacting'. Examples of these in an Android system are toast notifications (ambient display), heads up notifications (in the status bar), and pop up notifications, respectively. Ideally, each level will be available to cater to a user's personal preference. Information about a certain context should by default be found in the location where users would naturally look for\u00a0it.In feedback, how should different senses be addressed, to what level, where should it be shown, for how long, etc. These are aspects which should ideally be user configurable, with reasonable defaults. This should cater to what each user determines is\u00a0important.This concept includes the user's knowledge about how they feature in the system, how others feature with regards to the user's personal data, as well as what capabilities and constraints entities are given. The level of information and notification to convey depends on the user, as some will want more detail than others - meeting this balance will make the user more comfortable with their\u00a0involvement.This awareness can be divided amongst the three domains:\n- Social: Notable usage patterns on access, being able to correlate this with others and encourage better decisions.\n- Technical: Understanding the limitations of the system, and the capabilities if used correctly, to use the system more effectively. Users should understand the flow, state, and history of their personal data in the system.\n- Physical: Having regard for the repercussions of their physical state, including location, being perceivable by the\u00a0system.In maintaining awareness, one difficulty is in adequately informing users of the flows, history, and states of more complex systems. Meeting the balance between overwhelming users and underwhelming them can be\u00a0difficult.In a ubiquitous system, interpersonal information is something which should ideally be traceable to show who can access, and has accessed, what. In order for social governance to take place, people should be held accountable for what they do. When personal data is accessed, it should be clear who did so, and when - to both the person concerned and the one doing the accessing. Other matters such as how it was accessed, where from, or why, are also subject to the social norms and contexts placed on these aspects by those concerned. This 'you-know-that-I-know-that-you-know' effect controls the (mis)use of shared personal\u00a0information.Another balance to make is how much accountability is necessary. Too much exposure of usage may create tension, while too little may do the same. Being able to reliably link usage to an individual is also a matter to\u00a0consider.The social norms brought into the foreground, or created through the previous steps will bring about changes in usage. Being able to anticipate repercussions for actions, will cause users to think more carefully about what they reveal and what they access, as well as how (often), when, why, etc. If a user can determine that certain changes to information flow will be overall beneficial, the user may decide to act on those changes. This applies to both the user's needs, and that of the those around\u00a0them.Understanding the resulting effect of sharing or controlling information will help users find a level which suits them. Although some may retreat in-wards upon realizing the consequences of over-sharing, or over-share when the consequences of doing so are not immediately apparent. This is why meeting the balance is important. There is also the matter of outliers, where some users will not be as comfortable or as uncomfortable as the majority. Therefore, having the means to share less or more than others is\u00a0important.The right balance needs to be met, both in the selected defaults and in the minimum and maximum levels available to the users in settings. The settings should be found easily, as should additional information. Balanced defaults can be determined from identified norms among users, while minimums should cater to the least interested and maximums to those most interested in their data's\u00a0usage.Users should ideally be able to choose the medium for the notifications, and for information retrieval, which best suits them. Candidates include email, push notifications on mobile devices, or simply from the same interface as the rest of the\u00a0system.An effort can be made to slowly introduce users to this system. For example, starting out more privately and then gradually revealing future information so that users have time to adjust their usage. This way users are less likely to portray an undesirable usage\u00a0pattern.These correlations may also be stored in a secure way, so that they cannot be viewed arbitrarily by backend users. If users are given assurances about what information can be seen by who, including backend users, they will be more willing to make use of the information and notification\u00a0system.Disadvantages:\n- Initial discovery of the way they appear from the outside may lead users to retreat into themselves and disclose little to no information, cease using the system, and or call for their usage to be erased. This can be mitigated by slowly introducing users to the system without immediately providing intricate usage history.\n- Even if introduced slowly, users may be dissatisfied with their usage pattern as it appears to the system (and any authorised backend users). These correlations should ideally be difficult (or impossible) to retrieve if not by the user in\u00a0question.History: This example logs all access to the shared calendars by the group members, and if the calendar is public, especially the users which are not expected to do so. The sharing of these calendars produces social norm information, that is, social trends from how members use the information, spread it, or dismiss it. This includes the usual when/how/what/etc. information around access. As the members are presented this analysis, they are given the ability to react to it, and adjust the sharing or details within their calendars to their privacy\u00a0needs.Feedback: Augur informs users who accessed the calendar, when, where, and what in particular was seen. This is especially useful for shared calendars since the feedback mechanism allows users of the calendar to adjust what they add to it, or who is permitted access the\u00a0information.While a GCS notification, or information display could reside anywhere, the 'native habitat' for calendar related information is in the calendar application, a mail application, or if the user chooses and if necessary, the area where time sensitive notifications usually\u00a0appear.Awareness: Users are given information at the chosen detail and notification levels in order to feel comfortable with the system. Since monitoring can negatively impact the users, the level of this is also\u00a0configurable.Accountability: Social norms around mutual understanding of what will/has been accessed in this example will affect calendar viewing and sharing. Prying into other's affairs without reasonable explanation could have social (or other) consequences. This includes a distinction between occasional viewing and constant checks. This may result in less information being shared, different access control settings, or an inquiry into the usage, which may address underlying\u00a0issues.Change: Being able to see cause and effect around different personal data sharing, hiding, or specific information flows, will likely bring about changes in how users use the shared calendar system. Important information may be shared while leaving less important details out, increasing\u00a0efficiency.D. H. Nguyen and E. D. Mynatt, 'Privacy Mirrors : Understanding and Shaping Socio-technical Ubiquitous Computing Systems', pp. 1--18,\u00a02002.E. S. Chung et al., 'Development and Evaluation of Emerging Design Patterns for Ubiquitous Computing', DIS '04 Proceedings of the 5th conference on Designing interactive systems: processes, practices, methods, and techniques, pp. 233--242,\u00a02004.H. Baraki et al., Towards Interdisciplinary Design Patterns for Ubiquitous Computing Applications. Kassel, Germany,\u00a02014.G. Iachello and J. Hong, 'End-User Privacy in Human-Computer Interaction', Foundations and Trends in Human-Computer Interaction, vol. 1, no. 1, pp. 1--137,\u00a02007.", "tags": "", "title": "Privacy Mirrors"},{"url": "/patterns/Informed-Consent-for-Web-based-Transactions", "text": "User data is frequently collected for various purposes. Sometimes this data is personal, personally identifying, or otherwise sensitive. The data may serve to improve a service (or product) offered by a controller, or to provide relevant suggestions or advertisements to users. This is particularly prevalent on the web, as many websites derive most of their income from this data. Where income is instead in the form of purchase, user data is nonetheless needed to provide billing or shipping information. This includes auditing, logging, or other non-repudiation purposes to facilitate\u00a0transactions.Before collecting data, controllers must make sure users provide informed\u00a0consent.Controllers need to be able to inform their users about these purposes and means before the user\u00a0consents.Provide the user with clear and concise information regarding what may be learned from their data, and how that data can be used to offer or improve the service. Then acquire their explicit, freely-given\u00a0consent.To the extent possible given the limits imposed by web technology, provide the user with the six elements of informed consent: Disclosure [of purpose specification and limitation,] Agreement [and disagreement capabilities,] Comprehension [through easily understandable, comprehensive\u00a0and concise explanations,] Voluntariness [showing that consent is freely-given,] Competence [to make reasonable legally binding decisions, and] Minimal Distraction [which may otherwise aggravate the\u00a0user].Based\u00a0on:S. Romanosky, A. Acquisti, J. Hong, L. F. Cranor, and B. Friedman, \u201cPrivacy patterns for online interactions,\u201d Proceedings of the 2006 conference on Pattern languages of programs - PLoP \u201906, p. 1,\u00a02006.Fischer-H\u00fcbner, S., K\u00f6ffel, C., Pettersson, J.-S., Wolkerstorfer, P., Graf, C., Holtz, L. E., \u2026 Kellermann, B. (2010). HCI Pattern Collection \u2013 Version\u00a02.C. Bier and E. Krempel, \u201cCommon Privacy Patterns in Video Surveillance and Smart Energy,\u201d in ICCCT-2012, 2012, pp.\u00a0610\u2013615.C. Graf, P. Wolkerstorfer, A. Geven, and M. Tscheligi, \u201cA Pattern Collection for Privacy Enhancing Technology,\u201d The Second International Conferences of Pervasive Patterns and Applications (Patterns 2010), vol. 2, no. 1, pp. 72\u201377,\u00a02010.", "tags": "", "title": "Informed Consent for Web-based Transactions"},{"url": "/patterns/Dynamic-Privacy-Policy-Display", "text": "Controllers are mandated by various laws and regulations to ensure that users, their data subjects, are adequately informed before requesting consent. Failing this, the consent loses legitimacy and the controller may face repercussions. However, the main mechanism for supplying this information resides with privacy policies, which must also conform to legislative norms. The language this necessitates is long and complex, which jeopardizes the chances of users understanding it. This information can be summarized, and otherwise reworded to make the content more accessible to users, though typically the length of such summaries are still quite\u00a0long.Not all contexts are suitable for extensive privacy policy information, yet users often still need to be able to obtain additional data without breaking those\u00a0contexts.Provide the user with additional relevant policy information on hover or tap, by way of 'tooltips', to best inform them given contextual limitations. In a mobile setting these tooltips may unobtrusively become available to tap when the relevant control is most in focus (i.e. selected, centered, or occupies most of the\u00a0screen).This information may highlight the nature and potential consequences of the disclosure, and should be displayed\u00a0consistently.The information should be provided to the user where it is needed. Therefore the tooltip should appear on demand (i.e., need of information). This could be for example in a login dialog as soon as the user navigates the mouse into the concerning part of the interface. The tooltip should then be made visible to the user and contain all necessary information for making an informed\u00a0decision.By displaying the relevant [information pertaining to] privacy policies whenever they apply to what the user is currently doing or about to do, the user's awareness of what will happen with the information they're about to share is\u00a0increased.However, users may also happen to not trigger the tooltip, especially when using a mobile device. As such it is important that they are aware of its existence, and its importance, given the current\u00a0context.When a user needs to login and is given numerous options, with limited space provided, each option can have an assigned tooltip. These can appear on hover, tap, or scroll, where necessary appearing with less opacity until the user taps the tooltip itself. It can also lead to further detail through '(see more)' in a recognizable blue underlined hyperlink format. To encourage use of this a variant may either scroll through detail or show a visible scroll bar. Not needing the user to leave the application or webpage will require less effort on their\u00a0part.The PrimeLife HCI Pattern Collection (V2) features a prototype using tooltips to convey policy information on\u00a0hover. S. Fischer-H\u00fcbner, C. K\u00f6ffel, J.-S. Pettersson, P. Wolkerstorfer, C. Graf, L. E. Holtz, U. K\u00f6nig, H. Hedbom, and B. Kellermann, \u201cHCI Pattern Collection - Version 2,\u201d\u00a02010.C. Graf, P. Wolkerstorfer, A. Geven, and M. Tscheligi, \u201cA Pattern Collection for Privacy Enhancing Technology,\u201d The Second International Conferences of Pervasive Patterns and Applications (Patterns 2010), vol. 2, no. 1, pp. 72\u201377,\u00a02010.", "tags": "", "title": "Dynamic Privacy Policy Display"},{"url": "/patterns/Personal-data-store", "text": "Subjects keep control on their personal data that are stored on a\npersonal\u00a0device.The pattern is applicable to any data produced by the data subject (or\noriginally under his control) as opposed to data about him produced by\nthird\u00a0parties.Data subjects actually lose control over their data when they are\nstored on a server operated by a third\u00a0party.A solution consists in combining a central server and secure personal\ntokens. Personal tokens, which can take the form of USB keys, embed a\ndatabase system, a local web server and a certificate for their\nauthentication by the central server. Data subjects can decide on the\nstatus of their data and, depending on their level of sensitivity,\nchoose to record them exclusively on their personal token or to have\nthem replicated on the central server. Replication on the central\nserver is useful to enhance sustainability and to allow designated\nthird parties (e.g. health professionals) to get access to the\u00a0data.Enhance the control of the subjects on their personal\u00a0data.Data subjects need to be equipped with a personal data\u00a0store.Patients want to keep control over their health data but also to grant\nspecific access to some health\u00a0professionals.It has even been deployed for certain types of services, in\nparticular, in the health\u00a0sector.", "tags": "", "title": "Personal Data Store"}]}